<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<h1>History of computing
in computer</h1>
<style>
html{
   
font-size: 20px;
font-color:white
background-color: rgb(14, 186, 249); colour
color: white;
font-style:calibri
}
#test{
background-color: aqua;
color: rgb(252, 6, 6);
font-size: 25px;
font-color:white
text-align: justify;
}
</style>
<body>
<p>
A computer might be described with deceptive simplicity as “an apparatus that performs routine calculations
automatically.” Such a definition would owe its deceptiveness to a naive and narrow view of calculation as a strictly
mathematical process. In fact, calculation underlies many activities that are not normally thought of as mathematical.
Walking across a room, for instance, requires many complex, albeit subconscious, calculations. Computers, too, have
proved capable of solving a vast array of problems, from balancing a checkbook to even—in the form of guidance systems
for robots—walking across a room.

Before the true power of computing could be realized, therefore, the naive view of calculation had to be overcome. The
inventors who labored to bring the computer into the world had to learn that the thing they were inventing was not just
a number cruncher, not merely a calculator. For example, they had to learn that it was not necessary to invent a new
computer for every new calculation and that a computer could be designed to solve numerous problems, even problems not
yet imagined when the computer was built. They also had to learn how to tell such a general problem-solving computer
what problem to solve. In other words, they had to invent programming.

They had to solve all the heady problems of developing such a device, of implementing the design, of actually building
the thing. The history of the solving of these problems is the history of the computer. That history is covered in this
section, and links are provided to entries on many of the individuals and companies mentioned. In addition, see the
articles computer science and supercomputer.

Early history
Computer precursors
The abacus
The earliest known calculating device is probably the abacus. It dates back at least to 1100 bce and is still in use
today, particularly in Asia. Now, as then, it typically consists of a rectangular frame with thin parallel rods strung
with beads. Long before any systematic positional notation was adopted for the writing of numbers, the abacus assigned
different units, or weights, to each rod. This scheme allowed a wide range of numbers to be represented by just a few
beads and, together with the invention of zero in India, may have inspired the invention of the Hindu-Arabic number
system. In any case, abacus beads can be readily manipulated to perform the common arithmetical operations—addition,
subtraction, multiplication, and division—that are useful for commercial transactions and in bookkeeping.

The abacus is a digital device; that is, it represents values discretely. A bead is either in one predefined position or
another, representing unambiguously, say, one or zero.

Analog calculators: from Napier’s logarithms to the slide rule
Calculating devices took a different turn when John Napier, a Scottish mathematician, published his discovery of
logarithms in 1614. As any person can attest, adding two 10-digit numbers is much simpler than multiplying them
together, and the transformation of a multiplication problem into an addition problem is exactly what logarithms enable.
This simplification is possible because of the following logarithmic property: the logarithm of the product of two
numbers is equal to the sum of the logarithms of the numbers. By 1624, tables with 14 significant digits were available
for the logarithms of numbers from 1 to 20,000, and scientists quickly adopted the new labor-saving tool for tedious
astronomical calculations.

Most significant for the development of computing, the transformation of multiplication into addition greatly simplified
the possibility of mechanization. Analog calculating devices based on Napier’s logarithms—representing digital values
with analogous physical lengths—soon appeared. In 1620 Edmund Gunter, the English mathematician who coined the terms
cosine and cotangent, built a device for performing navigational calculations: the Gunter scale, or, as navigators
simply called it, the gunter. About 1632 an English clergyman and mathematician named William Oughtred built the first
slide rule, drawing on Napier’s ideas. That first slide rule was circular, but Oughtred also built the first rectangular
one in 1633. The analog devices of Gunter and Oughtred had various advantages and disadvantages compared with digital
devices such as the abacus. What is important is that the consequences of these design decisions were being tested in
the real world.

Digital calculators: from the Calculating Clock to the Arithmometer
Calculating Clock
Calculating Clock A reproduction of Wilhelm Schickard's Calculating Clock. The device could add and subtract six-digit
numbers (with a bell for seven-digit overflows) through six interlocking gears, each of which turned one-tenth of a
rotation for each full rotation of the gear to its right. Thus, 10 rotations of any gear would produce a “carry” of one
digit on the following gear and change the corresponding display.
In 1623 the German astronomer and mathematician Wilhelm Schickard built the first calculator. He described it in a
letter to his friend the astronomer Johannes Kepler, and in 1624 he wrote again to explain that a machine he had
commissioned to be built for Kepler was, apparently along with the prototype, destroyed in a fire. He called it a
Calculating Clock, which modern engineers have been able to reproduce from details in his letters. Even general
knowledge of the clock had been temporarily lost when Schickard and his entire family perished during the Thirty Years’
War.

But Schickard may not have been the true inventor of the calculator. A century earlier, Leonardo da Vinci sketched plans
for a calculator that were sufficiently complete and correct for modern engineers to build a calculator on their basis.

Arithmetic Machine, or Pascaline
Arithmetic Machine, or Pascaline The Arithmetic Machine, or Pascaline, a French monetary (nondecimal) calculator
designed by Blaise Pascal c. 1642. Numbers could be added by turning the wheels (located along the bottom of the
machine) clockwise and subtracted by turning the wheels counterclockwise. Each digit in the answer was displayed in a
separate window, visible at the top of the photograph.
The first calculator or adding machine to be produced in any quantity and actually used was the Pascaline, or Arithmetic
Machine, designed and built by the French mathematician-philosopher Blaise Pascal between 1642 and 1644. It could only
do addition and subtraction, with numbers being entered by manipulating its dials. Pascal invented the machine for his
father, a tax collector, so it was the first business machine too (if one does not count the abacus). He built 50 of
them over the next 10 years.

Step Reckoner
Step Reckoner A reproduction of Gottfried Wilhelm von Leibniz's Step Reckoner, from the original located in the Trinks
Brunsviga Museum at Hannover, Germany. Turning the crank (left) rotated several drums, each of which turned a gear
connected to a digital counter.
In 1671 the German mathematician-philosopher Gottfried Wilhelm von Leibniz designed a calculating machine called the
Step Reckoner. (It was first built in 1673.) The Step Reckoner expanded on Pascal’s ideas and did multiplication by
repeated addition and shifting.

Leibniz was a strong advocate of the binary number system. Binary numbers are ideal for machines because they require
only two digits, which can easily be represented by the on and off states of a switch. When computers became electronic,
the binary system was particularly appropriate because an electrical circuit is either on or off. This meant that on
could represent true, off could represent false, and the flow of current would directly represent the flow of logic.

Leibniz was prescient in seeing the appropriateness of the binary system in calculating machines, but his machine did
not use it. Instead, the Step Reckoner represented numbers in decimal form, as positions on 10-position dials. Even
decimal representation was not a given: in 1668 Samuel Morland invented an adding machine specialized for British
money—a decidedly nondecimal system.

Pascal’s, Leibniz’s, and Morland’s devices were curiosities, but with the Industrial Revolution of the 18th century came
a widespread need to perform repetitive operations efficiently. With other activities being mechanized, why not
calculation? In 1820 Charles Xavier Thomas de Colmar of France effectively met this challenge when he built his
Arithmometer, the first commercial mass-produced calculating device. It could perform addition, subtraction,
multiplication, and, with some more elaborate user involvement, division. Based on Leibniz’s technology, it was
extremely popular and sold for 90 years. In contrast to the modern calculator’s credit-card size, the Arithmometer was
large enough to cover a desktop.

The Jacquard loom
Calculators such as the Arithmometer remained a fascination after 1820, and their potential for commercial use was well
understood. Many other mechanical devices built during the 19th century also performed repetitive functions more or less
automatically, but few had any application to computing. There was one major exception: the Jacquard loom, invented in
1804–05 by a French weaver, Joseph-Marie Jacquard.

Jacquard loom
Jacquard loomJacquard loom, engraving, 1874. At the top of the machine is a stack of punched cards that would be fed
into the loom to control the weaving pattern. This method of automatically issuing machine instructions was employed by
computers well into the 20th century.
The Jacquard loom was a marvel of the Industrial Revolution. A textile-weaving loom, it could also be called the first
practical information-processing device. The loom worked by tugging various-colored threads into patterns by means of an
array of rods. By inserting a card punched with holes, an operator could control the motion of the rods and thereby
alter the pattern of the weave. Moreover, the loom was equipped with a card-reading device that slipped a new card from
a pre-punched deck into place every time the shuttle was thrown, so that complex weaving patterns could be automated.

What was extraordinary about the device was that it transferred the design process from a labor-intensive weaving stage
to a card-punching stage. Once the cards had been punched and assembled, the design was complete, and the loom
implemented the design automatically. The Jacquard loom, therefore, could be said to be programmed for different
patterns by these decks of punched cards.

For those intent on mechanizing calculations, the Jacquard loom provided important lessons: the sequence of operations
that a machine performs could be controlled to make the machine do something quite different; a punched card could be
used as a medium for directing the machine; and, most important, a device could be directed to perform different tasks
by feeding it instructions in a sort of language—i.e., making the machine programmable.

It is not too great a stretch to say that, in the Jacquard loom, programming was invented before the computer. The close
relationship between the device and the program became apparent some 20 years later, with Charles Babbage’s invention of
the first computer.

The first computer
By the second decade of the 19th century, a number of ideas necessary for the invention of the computer were in the air.
First, the potential benefits to science and industry of being able to automate routine calculations were appreciated,
as they had not been a century earlier. Specific methods to make automated calculation more practical, such as doing
multiplication by adding logarithms or by repeating addition, had been invented, and experience with both analog and
digital devices had shown some of the benefits of each approach. The Jacquard loom (as described in the previous
section, Computer precursors) had shown the benefits of directing a multipurpose device through coded instructions, and
it had demonstrated how punched cards could be used to modify those instructions quickly and flexibly. It was a
mathematical genius in England who began to put all these pieces together.

The Difference Engine
Difference Engine
Difference EngineThe completed portion of Charles Babbage's Difference Engine, 1832. This advanced calculator was
intended to produce logarithm tables used in navigation. The value of numbers was represented by the positions of the
toothed wheels marked with decimal numbers.
Charles Babbage was an English mathematician and inventor: he invented the cowcatcher, reformed the British postal
system, and was a pioneer in the fields of operations research and actuarial science. It was Babbage who first suggested
that the weather of years past could be read from tree rings. He also had a lifelong fascination with keys, ciphers, and
mechanical dolls.

As a founding member of the Royal Astronomical Society, Babbage had seen a clear need to design and build a mechanical
device that could automate long, tedious astronomical calculations. He began by writing a letter in 1822 to Sir Humphry
Davy, president of the Royal Society, about the possibility of automating the construction of mathematical
tables—specifically, logarithm tables for use in navigation. He then wrote a paper, “On the Theoretical Principles of
the Machinery for Calculating Tables,” which he read to the society later that year. (It won the Royal Society’s first
Gold Medal in 1823.) Tables then in use often contained errors, which could be a life-and-death matter for sailors at
sea, and Babbage argued that, by automating the production of the tables, he could assure their accuracy. Having gained
support in the society for his Difference Engine, as he called it, Babbage next turned to the British government to fund
development, obtaining one of the world’s first government grants for research and technological development.

Babbage approached the project very seriously: he hired a master machinist, set up a fireproof workshop, and built a
dustproof environment for testing the device. Up until then calculations were rarely carried out to more than 6 digits;
Babbage planned to produce 20- or 30-digit results routinely. The Difference Engine was a digital device: it operated on
discrete digits rather than smooth quantities, and the digits were decimal (0–9), represented by positions on toothed
wheels, rather than the binary digits that Leibniz favored (but did not use). When one of the toothed wheels turned from
9 to 0, it caused the next wheel to advance one position, carrying the digit just as Leibniz’s Step Reckoner calculator
had operated.

The Difference Engine was more than a simple calculator, however. It mechanized not just a single calculation but a
whole series of calculations on a number of variables to solve a complex problem. It went far beyond calculators in
other ways as well. Like modern computers, the Difference Engine had storage—that is, a place where data could be held
temporarily for later processing—and it was designed to stamp its output into soft metal, which could later be used to
produce a printing plate.

Nevertheless, the Difference Engine performed only one operation. The operator would set up all of its data registers
with the original data, and then the single operation would be repeatedly applied to all of the registers, ultimately
producing a solution. Still, in complexity and audacity of design, it dwarfed any calculating device then in existence.

The full engine, designed to be room-size, was never built, at least not by Babbage. Although he sporadically received
several government grants—governments changed, funding often ran out, and he had to personally bear some of the
financial costs—he was working at or near the tolerances of the construction methods of the day, and he ran into
numerous construction difficulties. All design and construction ceased in 1833, when Joseph Clement, the machinist
responsible for actually building the machine, refused to continue unless he was prepaid. (The completed portion of the
Difference Engine is on permanent exhibition at the Science Museum in London.)

The Analytical Engine
Charles Babbage: Analytical Engine
Charles Babbage: Analytical EngineA portion (completed 1910) of Charles Babbage's Analytical Engine. Only partially
built at the time of his death in 1871, this portion contains the “mill” (functionally analogous to a modern computer's
central processing unit) and a printing mechanism.
While working on the Difference Engine, Babbage began to imagine ways to improve it. Chiefly he thought about
generalizing its operation so that it could perform other kinds of calculations. By the time the funding had run out in
1833, he had conceived of something far more revolutionary: a general-purpose computing machine called the Analytical
Engine.

The Analytical Engine was to be a general-purpose, fully program-controlled, automatic mechanical digital computer. It
would be able to perform any calculation set before it. Before Babbage there is no evidence that anyone had ever
conceived of such a device, let alone attempted to build one. The machine was designed to consist of four components:
the mill, the store, the reader, and the printer. These components are the essential components of every computer today.
The mill was the calculating unit, analogous to the central processing unit (CPU) in a modern computer; the store was
where data were held prior to processing, exactly analogous to memory and storage in today’s computers; and the reader
and printer were the input and output devices.

As with the Difference Engine, the project was far more complex than anything theretofore built. The store was to be
large enough to hold 1,000 50-digit numbers; this was larger than the storage capacity of any computer built before
1960. The machine was to be steam-driven and run by one attendant. The printing capability was also ambitious, as it had
been for the Difference Engine: Babbage wanted to automate the process as much as possible, right up to producing
printed tables of numbers.

The reader was another new feature of the Analytical Engine. Data (numbers) were to be entered on punched cards, using
the card-reading technology of the Jacquard loom. Instructions were also to be entered on cards, another idea taken
directly from Jacquard. The use of instruction cards would make it a programmable device and far more flexible than any
machine then in existence. Another element of programmability was to be its ability to execute instructions in other
than sequential order. It was to have a kind of decision-making ability in its conditional control transfer, also known
as conditional branching, whereby it would be able to jump to a different instruction depending on the value of some
data. This extremely powerful feature was missing in many of the early computers of the 20th century.

By most definitions, the Analytical Engine was a real computer as understood today—or would have been, had not Babbage
run into implementation problems again. Actually building his ambitious design was judged infeasible given the current
technology, and Babbage’s failure to generate the promised mathematical tables with his Difference Engine had dampened
enthusiasm for further government funding. Indeed, it was apparent to the British government that Babbage was more
interested in innovation than in constructing tables.

All the same, Babbage’s Analytical Engine was something new under the sun. Its most revolutionary feature was the
ability to change its operation by changing the instructions on punched cards. Until this breakthrough, all the
mechanical aids to calculation were merely calculators or, like the Difference Engine, glorified calculators. The
Analytical Engine, although not actually completed, was the first machine that deserved to be called a computer.

Ada Lovelace, the first programmer
Ada Lovelace
Ada LovelacePortrait of Ada Lovelace by Margaret Carpenter, 1836.
The distinction between calculator and computer, although clear to Babbage, was not apparent to most people in the early
19th century, even to the intellectually adventuresome visitors at Babbage’s soirees—with the exception of a young girl
of unusual parentage and education.

Ada Lovelace's life and impact on scientific computing
Ada Lovelace's life and impact on scientific computingWalter Isaacson discussing the life and impact of Ada Lovelace.
See all videos for this article
Augusta Ada King, the countess of Lovelace, was the daughter of the poet Lord Byron and the mathematically inclined Anne
Millbanke. One of her tutors was Augustus De Morgan, a famous mathematician and logician. Because Byron was involved in
a notorious scandal at the time of her birth, Lovelace’s mother encouraged her mathematical and scientific interests,
hoping to suppress any inclination to wildness she may have inherited from her father.

Toward that end, Lovelace attended Babbage’s soirees and became fascinated with his Difference Engine. She also
corresponded with him, asking pointed questions. It was his plan for the Analytical Engine that truly fired her
imagination, however. In 1843, at age 27, she had come to understand it well enough to publish the definitive paper
explaining the device and drawing the crucial distinction between this new thing and existing calculators. The
Analytical Engine, she argued, went beyond the bounds of arithmetic. Because it operated on general symbols rather than
on numbers, it established “a link…between the operations of matter and the abstract mental processes of the most
abstract branch of mathematical science.” It was a physical device that was capable of operating in the realm of
abstract thought.

Lovelace rightly reported that this was not only something no one had built, it was something that no one before had
even conceived. She went on to become the world’s only expert on the process of sequencing instructions on the punched
cards that the Analytical Engine used; that is, she became the world’s first computer programmer.

One feature of the Analytical Engine was its ability to place numbers and instructions temporarily in its store and
return them to its mill for processing at an appropriate time. This was accomplished by the proper sequencing of
instructions and data in its reader, and the ability to reorder instructions and data gave the machine a flexibility and
power that was hard to grasp. The first electronic digital computers of a century later lacked this ability. It was
remarkable that a young scholar realized its importance in 1840, and it would be 100 years before anyone would
understand it so well again. In the intervening century, attention would be diverted to the calculator and other
business machines.

Early business machines
Throughout the 19th century, business machines were coming into common use. Calculators became available as a tool of
commerce in 1820 (see the earlier section Digital calculators), and in 1874 the Remington Arms Company, Inc., sold the
first commercially viable typewriter. Other machines were invented for other specific business tasks. None of these
machines was a computer, but they did advance the state of practical mechanical knowledge—knowledge that would be used
in computers later.

One of these machines was invented in response to a sort of constitutional crisis in the United States: the census
tabulator.

Herman Hollerith’s census tabulator
Hollerith census tabulator
Hollerith census tabulator This cover of Scientific American, August 30, 1890, displays various aspects of Herman
Hollerith's invention.
The U.S. Constitution mandates that a census of the population be performed every 10 years. The first attempt at any
mechanization of the census was in 1870, when statistical data were transcribed onto a rolling paper tape displayed
through a small slotted window. As the size of America’s population exploded in the 19th century and the number of
census questions expanded, the urgency of further mechanization became increasingly clear.

After graduating from the Columbia University School of Mines, New York City, in 1879, Herman Hollerith obtained his
first job with one of his former professors, William P. Trowbridge, who had received a commission as a special agent for
the 1880 census. It was while employed at the Census Office that Hollerith first saw the pressing need for automating
the tabulation of statistical data.

Over the next 10 years Hollerith refined his ideas, obtaining his first patent in 1884 for a machine to punch and count
cards. He then organized the health records for Baltimore, Maryland, for New York City, and for the state of New
Jersey—all in preparation for winning the contract to tabulate the 1890 U.S. Census. The success of the U.S. census
opened European governments to Hollerith’s machines. Most notably, a contract with the Russian government, signed on
December 15, 1896, may have induced him to incorporate as the Tabulating Machine Company on December 5, 1896.

Other early business machine companies
Improvements in calculators continued: by the 1880s they could add in the accumulation of partial results, store past
results, and print. Then, in 1892, William Seward Burroughs, who along with two other St. Louis, Missouri, businessmen
had started the American Arithmometer Company in 1886 in order to build adding machines, obtained a patent for one of
the first truly practical and commercially successful calculators. Burroughs died in 1898, and his company was
reorganized as the Burroughs Adding Machine Company in Detroit, Michigan, in 1905.

All the calculators—and virtually all the information-processing devices—sold at this time were designed for commercial
purposes, not scientific research. By the turn of the century, commercial calculating devices were in common use, as
were other special-purpose machines such as one that generated serial numbers for banknotes. As a result, many of the
business machine companies in the United States were doing well, including Hollerith’s Tabulating Machine Company.

In 1911 several of these companies combined to form the Computing-Tabulating-Recording Company, or CTR. In 1914 Thomas
J. Watson, Sr., left his sales manager position at the National Cash Register Company to become president of CTR, and 10
years later CTR changed its name to International Business Machines Corporation, or IBM. In the second half of the
century, IBM would become the giant of the world computer industry, but such commercial gains did not take place until
enormous progress had been made in the theoretical understanding of the modern computer during the remarkable decades of
the 1930s and ’40s. (This progress is described in the next section, Invention of the modern computer.)

Invention of the modern computer
Early experiments
As the technology for realizing a computer was being honed by the business machine companies in the early 20th century,
the theoretical foundations were being laid in academia. During the 1930s two important strains of computer-related
research were being pursued in the United States at two universities in Cambridge, Massachusetts. One strain produced
the Differential Analyzer, the other a series of devices ending with the Harvard Mark IV.

Vannevar Bush’s Differential Analyzer
Vannevar Bush with Differential Analyzer
Vannevar Bush with Differential AnalyzerVannevar Bush with his Differential Analyzer, c. 1935.
In 1930 an engineer named Vannevar Bush at the Massachusetts Institute of Technology (MIT) developed the first modern
analog computer. The Differential Analyzer, as he called it, was an analog calculator that could be used to solve
certain classes of differential equations, a type of problem common in physics and engineering applications that is
often very tedious to solve. Variables were represented by shaft motion, and addition and multiplication were
accomplished by feeding the values into a set of gears. Integration was carried out by means of a knife-edged wheel
rotating at a variable radius on a circular table. The individual mechanical integrators were then interconnected to
solve a set of differential equations.

The Differential Analyzer proved highly useful, and a number of them were built and used at various universities. Still
the device was limited to solving this one class of problem, and, as is the case for all analog devices, it produced
approximate, albeit practical, solutions. Nevertheless, important applications for analog computers and analog-digital
hybrid computers still exist, particularly for simulating complicated dynamical systems such as aircraft flight, nuclear
power plant operations, and chemical reactions.

Howard Aiken’s digital calculators
Harvard Mark I, 1943
Harvard Mark I, 1943Designed by Howard Aiken, this electromechanical computer, more than 15 meters (50 feet) long and
containing some 750,000 components, was used to make ballistics calculations during World War II.
While Bush was working on analog computing at MIT, across town Harvard professor Howard Aiken was working with digital
devices for calculation. He had begun to realize in hardware something like Babbage’s Analytical Engine, which he had
read about. Starting in 1937, he laid out detailed plans for a series of four calculating machines of increasing
sophistication, based on different technologies, from the largely mechanical Mark I to the electronic Mark IV.

Aiken was methodically exploring the technological advances made since the mechanical assembly and steam power available
to Babbage. Electromagnetic relay circuits were already being used in business machines, and the vacuum tube—a switch
with no moving parts, very high speed action, and greater reliability than electromechanical relays—was quickly put to
use in the early experimental machines.

The business machines of the time used plugboards (something like telephone switchboards) to route data manually, and
Aiken chose not to use them for the specification of instructions. This turned out to make his machine much easier to
program than the more famous ENIAC, designed somewhat later, which had to be manually rewired for each program.

From 1939 to 1944 Aiken, in collaboration with IBM, developed his first fully functional computer, known as the Harvard
Mark I. The machine, like Babbage’s, was huge: more than 15 meters (50 feet) long, weighing 4,500 kg (5 tons), and
consisting of about 750,000 separate parts, it was mostly mechanical. For input and output it used three paper-tape
readers, two card readers, a card punch, and two typewriters. It took between three and six seconds to add two numbers.
Aiken developed three more such machines (Mark II–IV) over the next few years and is credited with developing the first
fully automatic large-scale calculator.

The Turing machine
Alan Turing, while a mathematics student at the University of Cambridge, was inspired by German mathematician David
Hilbert’s formalist program, which sought to demonstrate that any mathematical problem can potentially be solved by an
algorithm—that is, by a purely mechanical process. Turing interpreted this to mean a computing machine and set out to
design one capable of resolving all mathematical problems, but in the process he proved in his seminal paper “On
Computable Numbers, with an Application to the Entscheidungsproblem [‘Halting Problem’]” (1936) that no such universal
mathematical solver could ever exist.

In order to design his machine (known to posterity as the “Turing machine”), he needed to find an unambiguous definition
of the essence of a computer. In doing so, Turing worked out in great detail the basic concepts of a universal computing
machine—that is, a computing machine that could, at least in theory, do anything that a special-purpose computing device
could do. In particular, it would not be limited to doing arithmetic. The internal states of the machine could represent
numbers, but they could equally well represent logic values or letters. In fact, Turing believed that everything could
be represented symbolically, even abstract mental states, and he was one of the first advocates of the
artificial-intelligence position that computers can potentially “think.”

Turing’s work up to this point was entirely abstract, entirely a theoretical demonstration. Nevertheless, he made it
clear from the start that his results implied the possibility of building a machine of the sort he described. His work
characterized the abstract essence of any computing device so well that it was in effect a challenge to actually build
one.

Turing’s work had an immediate effect on only a small number of academics at a few universities who were interested in
the concept of computing machinery. It had no immediate effect on the growing industry of business machines, all of
which were special-purpose devices. But to the few who were interested, Turing’s work was an inspiration to pursue
something of which most of the world had not even conceived: a universal computing machine.

Pioneering work
The Atanasoff-Berry Computer
Clifford Berry and Atanasoff-Berry Computer
Clifford Berry and Atanasoff-Berry ComputerClifford Berry and the Atanasoff-Berry Computer. The ABC, c. 1942, was
possibly the first electronic digital computer.
It was generally believed that the first electronic digital computers were the Colossus, built in England in 1943, and
the ENIAC, built in the United States in 1945. However, the first special-purpose electronic computer may actually have
been invented by John Vincent Atanasoff, a physicist and mathematician at Iowa State College (now Iowa State
University), during 1937–42. (Atanasoff also claimed to have invented the term analog computer to describe machines such
as Vannevar Bush’s Differential Analyzer.) Together with his graduate assistant Clifford E. Berry, Atanasoff built a
successful small prototype in 1939 for the purpose of testing two ideas central to his design: capacitors to store data
in binary form and electronic logic circuits to perform addition and subtraction. They then began the design and
construction of a larger, more general-purpose computer, known as the Atanasoff-Berry Computer, or ABC.

Various components of the ABC were designed and built from 1939 to 1942, but development was discontinued with the onset
of World War II. The ABC featured about 300 vacuum tubes for control and arithmetic calculations, use of binary numbers,
logic operations (instead of direct counting), memory capacitors, and punched cards as input/output units. (At
Atanasoff’s invitation, another early computer pioneer, John Mauchly, stayed at his home and was freely shown his work
for several days in June 1941. For more on the ramifications of this visit, see BTW: Computer patent wars.)

The first computer network
Between 1940 and 1946 George Stibitz and his team at Bell Laboratories built a series of machines with telephone
technologies—i.e., employing electromechanical relays. These were the first machines to serve more than one user and the
first to work remotely over telephone lines. However, because they were based on slow mechanical relays rather than
electronic switches, they became obsolete almost as soon as they were constructed.

Konrad Zuse
Meanwhile, in Germany, engineer Konrad Zuse had been thinking about calculating machines. He was advised by a calculator
manufacturer in 1937 that the field was a dead end and that every computing problem had already been solved. Zuse had
something else in mind, though.

For one thing, Zuse worked in binary from the beginning. All of his prototype machines, built in 1936, used binary
representation in order to simplify construction. This had the added advantage of making the connection with logic
clearer, and Zuse worked out the details of how the operations of logic (e.g., AND, OR, and NOT) could be mapped onto
the design of the computer’s circuits. (English mathematician George Boole had shown the connection between logic and
mathematics in the mid-19th century, developing an algebra of logic now known as Boolean algebra.) Zuse also spent more
time than his predecessors and contemporaries developing software for his computer, the language in which it was to be
programmed. (His contributions to programming are examined in the section Programming languages.) Although all his early
prewar machines were really calculators—not computers—his Z3, completed in December 1941 (and destroyed on April 6,
1945, during an Allied air raid on Berlin), was the first program-controlled processor.

Because all Zuse’s work was done in relative isolation, he knew little about work on computers in the United States and
England, and, when the war began, the isolation became complete.

The following section, Developments during World War II, examines the development during the 1940s of the first fully
functional digital computers.

Developments during World War II
Colossus
How did the world's first programmable electronic computer work?
How did the world's first programmable electronic computer work?An overview of Colossus, the world's first large-scale
electronic computer.
See all videos for this article
The exigencies of war gave impetus and funding to computer research. For example, in Britain the impetus was code
breaking. The Ultra project was funded with much secrecy to develop the technology necessary to crack ciphers and codes
produced by the German electromechanical devices known as the Enigma and the Geheimschreiber (“Secret Writer”). The
first in a series of important code-breaking machines, Colossus, also known as the Mark I, was built under the direction
of Sir Thomas Flowers and delivered in December 1943 to the code-breaking operation at Bletchley Park, a government
research center north of London. It employed approximately 1,800 vacuum tubes for computations. Successively larger and
more elaborate versions were built over the next two years.

The Ultra project had a gifted mathematician associated with the Bletchley Park effort, and one familiar with codes.
Alan Turing, who had earlier articulated the concept of a universal computing device (described in the section The
Turing machine), may have pushed the project farther in the direction of a general-purpose device than his government
originally had in mind. Turing’s advocacy helped keep up government support for the project.

Colossus computer
Colossus computerThe Colossus computer at Bletchley Park, Buckinghamshire, England, c. 1943. Funding for this
code-breaking machine came from the Ultra project.
Although it lacked some characteristics now associated with computers, Colossus can plausibly be described as the first
electronic digital computer, and it was certainly a key stepping stone to the development of the modern computer.
Although Colossus was designed to perform specific cryptographic-related calculations, it could be used for
more-generalized purposes. Its design pioneered the massive use of electronics in computation, and it embodied an
insight from Flowers of the importance of storing data electronically within the machine. The operation at Bletchley
foreshadowed the modern data center.

Colossus was successful in its intended purpose: the German messages it helped to decode provided information about
German battle orders, supplies, and personnel; it also confirmed that an Allied deception campaign, Operation Fortitude,
was working.

The series of Colossus computers were disassembled after the war, and most information about them remained classified
until the 1990s. In 1996 the basic Colossus machine was rebuilt and switched on at Bletchley Park.

The Z4
In Germany, Konrad Zuse began construction of the Z4 in 1943 with funding from the Air Ministry. Like his Z3 (described
in the section Konrad Zuse), the Z4 used electromechanical relays, in part because of the difficulty in acquiring the
roughly 2,000 necessary vacuum tubes in wartime Germany. The Z4 was evacuated from Berlin in early 1945, and it
eventually wound up in Hinterstein, a small village in the Bavarian Alps, where it remained until Zuse brought it to the
Federal Technical Institute in Zürich, Switzerland, for refurbishing in 1950. Although unable to continue with hardware
development, Zuse made a number of advances in software design.

Zuse’s use of floating-point representation for numbers—the significant digits, known as the mantissa, are stored
separately from a pointer to the decimal point, known as the exponent, allowing a very large range of numbers to be
handled—was far ahead of its time. In addition, Zuse developed a rich set of instructions, handled infinite values
correctly, and included a “no-op”—that is, an instruction that did nothing. Only significant experience in programming
would show the need for something so apparently useless.

The Z4’s program was punched on used movie film and was separate from the mechanical memory for data (in other words,
there was no stored program). The machine was relatively reliable (it normally ran all night unattended), but it had no
decision-making ability. Addition took 0.5 to 1.25 seconds, multiplication 3.5 seconds.

ENIAC
In the United States, government funding went to a project led by John Mauchly, J. Presper Eckert, Jr., and their
colleagues at the Moore School of Electrical Engineering at the University of Pennsylvania; their objective was an
all-electronic computer. Under contract to the army and under the direction of Herman Goldstine, work began in early
1943 on the Electronic Numerical Integrator and Computer (ENIAC). The next year, mathematician John von Neumann, already
on full-time leave from the Institute for Advanced Studies (IAS), Princeton, New Jersey, for various government research
projects (including the Manhattan Project), began frequent consultations with the group.

ENIAC was something less than the dream of a universal computer. Designed for the specific purpose of computing values
for artillery range tables, it lacked some features that would have made it a more generally useful machine. Like
Colossus but unlike Howard Aiken’s machine (described in the section Early experiments), it used plugboards for
communicating instructions to the machine; this had the advantage that, once the instructions were thus “programmed,”
the machine ran at electronic speed. Instructions read from a card reader or other slow mechanical device would not have
been able to keep up with the all-electronic ENIAC. The disadvantage was that it took days to rewire the machine for
each new problem. This was such a liability that only with some generosity could it be called programmable.

Nevertheless, ENIAC was the most powerful calculating device built to date. Like Charles Babbage’s Analytical Engine and
the Colossus, but unlike Aiken’s Mark I, Konrad Zuse’s Z4, and George Stibitz’s telephone-savvy machine, it did have
conditional branching—that is, it had the ability to execute different instructions or to alter the order of execution
of instructions based on the value of some data. (For instance, IF X > 5 THEN GO TO LINE 23.) This gave ENIAC a lot of
flexibility and meant that, while it was built for a specific purpose, it could be used for a wider range of problems.

ENIAC was enormous. It occupied the 15-by-9-meter (50-by-30-foot) basement of the Moore School, where its 40 panels were
arranged, U-shaped, along three walls. Each of the units was about 0.6 meter wide by 0.6 meter deep by 2.4 meters high
(2 by 2 by 8 feet). With approximately 18,000 vacuum tubes, 70,000 resistors, 10,000 capacitors, 6,000 switches, and
1,500 relays, it was easily the most complex electronic system theretofore built. ENIAC ran continuously (in part to
extend tube life), generating 150 kilowatts of heat, and could execute up to 5,000 additions per second, several orders
of magnitude faster than its electromechanical predecessors. Colossus, ENAIC, and subsequent computers employing vacuum
tubes are known as first-generation computers. (With 1,500 mechanical relays, ENIAC was still transitional to later,
fully electronic computers.)

Completed by February 1946, ENIAC had cost the government $400,000, and the war it was designed to help win was over.
Its first task was doing calculations for the construction of a hydrogen bomb. A portion of the machine is on exhibit at
the Smithsonian Institution in Washington, D.C.

Toward the classical computer
Bigger brains
The computers built during the war were built under unusual constraints. The British work was largely focused on code
breaking, the American work on computing projectile trajectories and calculations for the atomic bomb. The computers
were built as special-purpose devices, although they often embodied more general-purpose computing capabilities than
their specifications called for. The vacuum tubes in these machines were not entirely reliable, but with no moving parts
they were more reliable than the electromechanical switches they replaced, and they were much faster. Reliability was an
issue, since Colossus used some 1,500 tubes and ENIAC on the order of 18,000. But ENIAC was, by virtue of its electronic
realization, 1,000 times faster than the Harvard Mark I. Such speed meant that the machine could perform calculations
that were theretofore beyond human ability. Although tubes were a great advance over the electromechanical realization
of Aiken or the steam-and-mechanical model of Babbage, the basic architecture of the machines (that is, the functions
they were able to perform) was not much advanced beyond Babbage’s Difference Engine and Analytical Engine. In fact, the
original name for ENIAC was Electronic Difference Analyzer, and it was built to perform much like Babbage’s Difference
Engine.

After the war, efforts focused on fulfilling the idea of a general-purpose computing device. In 1945, before ENIAC was
even finished, planning began at the Moore School for ENIAC’s successor, the Electronic Discrete Variable Automatic
Computer, or EDVAC. (Planning for EDVAC also set the stage for an ensuing patent fight; see BTW: Computer patent wars.)
ENIAC was hampered, as all previous electronic computers had been, by the need to use one vacuum tube to store each bit,
or binary digit. The feasible number of vacuum tubes in a computer also posed a practical limit on storage
capacity—beyond a certain point, vacuum tubes are bound to burn out as fast as they can be changed. For EDVAC, Eckert
had a new idea for storage.

In 1880 French physicists Pierre and Jacques Curie had discovered that applying an electric current to a quartz crystal
would produce a characteristic vibration and vice versa. During the 1930s at Bell Laboratories, William Shockley, later
coinventor of the transistor, had demonstrated a device—a tube, called a delay line, containing water and ethylene
glycol—for effecting a predictable delay in information transmission. Eckert had already built and experimented in 1943
with such a delay line (using mercury) in conjunction with radar research, and sometime in 1944 he hit upon the new idea
of placing a quartz crystal at each end of the mercury delay line in order to sustain and modify the resulting pattern.
In effect, he invented a new storage device. Whereas ENIAC required one tube per bit, EDVAC could use a delay line and
10 vacuum tubes to store 1,000 bits. Before the invention of the magnetic core memory and the transistor, which would
eliminate the need for vacuum tubes altogether, the mercury delay line was instrumental in increasing computer storage
and reliability.

Von Neumann’s “Preliminary Discussion”
But the design of the modern, or classical, computer did not fully crystallize until the publication of a 1946 paper by
Arthur Burks, Herman Goldstine, and John von Neumann titled “Preliminary Discussion of the Logical Design of an
Electronic Computing Instrument.” Although the paper was essentially a synthesis of ideas currently “in the air,” it is
frequently cited as the birth certificate of computer science.

Among the principles enunciated in the paper were that data and instructions should be kept in a single store and that
instructions should be encoded so as to be modifiable by other instructions. This was an extremely critical decision,
because it meant that one program could be treated as data by another program. Zuse had considered and rejected this
possibility as too dangerous. But its inclusion by von Neumann’s group made possible high-level programming languages
and most of the advances in software of the following 50 years. Subsequently, computers with stored programs would be
known as von Neumann machines.

One problem that the stored-program idea solved was the need for rapid access to instructions. Colossus and ENIAC had
used plugboards, which had the advantage of enabling the instructions to be read in electronically, rather than by much
slower mechanical card readers, but it also had the disadvantage of making these first-generation machines very hard to
program. But if the instructions could be stored in the same electronic memory that held the data, they could be
accessed as quickly as needed. One immediately obvious consequence was that EDVAC would need a lot more memory than
ENIAC.

The first stored-program machines
Manchester Mark I
Manchester Mark IThe Manchester Mark I, the first stored-program digital computer, c. 1949.
Government secrecy hampered British efforts to build on wartime computer advances, but engineers in Britain still beat
the Americans to the goal of building the first stored-program digital computer. At the University of Manchester,
Frederic C. Williams and Tom Kilburn built a simple stored-program computer, known as the Baby, in 1948. This was built
to test their invention of a way to store information on a cathode-ray tube that enabled direct access (in contrast to
the mercury delay line’s sequential access) to stored information. Although faster than Eckert’s storage method, it
proved somewhat unreliable. Nevertheless, it became the preferred storage method for most of the early computers
worldwide that were not already committed to mercury delay lines.

Ferranti Mark I
Ferranti Mark ITom Kilburn standing beside the console of the Ferranti Mark I computer, c. 1950.
By 1949 Williams and Kilburn had extended the Baby to a full-size computer, the Manchester Mark I. This had two major
new features that were to become computer standards: a two-level store and instruction modification registers (which
soon evolved into index registers). A magnetic drum was added to provide a random-access secondary storage device. Until
machines were fitted with index registers, every instruction that referred to an address that varied as the program
ran—e.g., an array element—had to be preceded by instructions to alter its address to the current required value. Four
months after the Baby first worked, the British government contracted the electronics firm of Ferranti to build a
production computer based on the prospective Mark I. This became the Ferranti Mark I—the first commercial computer—of
which nine were sold.

Kilburn, Williams, and colleagues at Manchester also came up with a breakthrough that would revolutionize how a computer
executed instructions: they made it possible for the address portion of an instruction to be modified while the program
was running. Before this, an instruction specified that a particular action—say, addition—was to be performed on data in
one or more particular locations. Their innovation allowed the location to be modified as part of the operation of
executing the instruction. This made it very easy to address elements within an array sequentially.

EDSAC computer
EDSAC computerThe EDSAC computer, 1947, with designer Maurice Wilkes (kneeling in the center of the photograph).
At the University of Cambridge, meanwhile, Maurice Wilkes and others built what is recognized as the first full-size,
fully electronic, stored-program computer to provide a formal computing service for users. The Electronic Delay Storage
Automatic Calculator (EDSAC) was built on the set of principles synthesized by von Neumann and, like the Manchester Mark
I, became operational in 1949. Wilkes built the machine chiefly to study programming issues, which he realized would
become as important as the hardware details.

Whirlwind
New hardware continued to be invented, though. In the United States, Jay Forrester of the Massachusetts Institute of
Technology (MIT) and Jan Aleksander Rajchman of the Radio Corporation of America came up with a new kind of memory based
on magnetic cores that was fast enough to enable MIT to build the first real-time computer, Whirlwind. A real-time
computer is one that can respond seemingly instantly to basic instructions, thus allowing an operator to interact with a
“running” computer.

UNIVAC
After leaving the Moore School, Eckert and Mauchly struggled to obtain capital to build their latest design, a computer
they called the Universal Automatic Computer, or UNIVAC. (In the meantime, they contracted with the Northrop Corporation
to build the Binary Automatic Computer, or BINAC, which, when completed in 1949, became the first American
stored-program computer.) The partners delivered the first UNIVAC to the U.S. Bureau of the Census in March 1951,
although their company, their patents, and their talents had been acquired by Remington Rand, Inc., in 1950. Although it
owed something to experience with ENIAC, UNIVAC was built from the start as a stored-program computer, so it was really
different architecturally. It used an operator keyboard and console typewriter for input and magnetic tape for all other
input and output. Printed output was recorded on tape and then printed by a separate tape printer.

The UNIVAC I was designed as a commercial data-processing computer, intended to replace the punched-card accounting
machines of the day. It could read 7,200 decimal digits per second (it did not use binary numbers), making it by far the
fastest business machine yet built. Its use of Eckert’s mercury delay lines greatly reduced the number of vacuum tubes
needed (to 5,000), thus enabling the main processor to occupy a “mere” 4.4 by 2.3 by 2.7 meters (14.5 by 7.5 by 9 feet)
of space. It was a true business machine, signaling the convergence of academic computational research with the office
automation trend of the late 19th and early 20th centuries. As such, it ushered in the era of “Big Iron”—or large,
mass-produced computing equipment.

The age of Big Iron
IBM 650 computer system
IBM 650 computer system Relatively inexpensive, compact, and easy to operate, the IBM 650 quickly became the most widely
used computer for business applications.
A snapshot of computer development in the early 1950s would have to show a number of companies and laboratories in
competition—technological competition and increasingly earnest business competition—to produce the few computers then
demanded for scientific research. Several computer-building projects had been launched immediately after the end of
World War II in 1945, primarily in the United States and Britain. These projects were inspired chiefly by a 1946
document, “Preliminary Discussion of the Logical Design of an Electronic Digital Computing Instrument,” produced by a
group working under the direction of mathematician John von Neumann of the Institute for Advanced Study at Princeton
University. The IAS paper, as von Neumann’s document became known, articulated the concept of the stored program—a
concept that has been called the single largest innovation in the history of the computer. (Von Neumann’s principles are
described earlier, in the section Toward the classical computer.) Most computers built in the years following the
paper’s distribution were designed according to its plan, yet by 1950 there were still only a handful of working
stored-program computers.

Business use at this time was marginal because the machines were so hard to use. Although computer makers such as
Remington Rand, the Burroughs Adding Machine Company, and IBM had begun building machines to the IAS specifications, it
was not until 1954 that a real market for business computers began to emerge. The IBM 650, delivered at the end of 1954
for colleges and businesses, was a decimal implementation of the IAS design. With this low-cost magnetic drum computer,
which sold for about $200,000 apiece (compared with about $1,000,000 for the scientific model, the IBM 701), IBM had a
hit, eventually selling about 1,800 of them. In addition, by offering universities that taught computer science courses
around the IBM 650 an academic discount program (with price reductions of up to 60 percent), IBM established a cadre of
engineers and programmers for their machines. (Apple later used a similar discount strategy in American grade schools to
capture a large proportion of the early microcomputer market.)

A snapshot of the era would also have to show what could be called the sociology of computing. The actual use of
computers was restricted to a small group of trained experts, and there was resistance to the idea that this group
should be expanded by making the machines easier to use. Machine time was expensive, more expensive than the time of the
mathematicians and scientists who needed to use the machines, and computers could process only one problem at a time. As
a result, the machines were in a sense held in higher regard than the scientists. If a task could be done by a person,
it was thought that the machine’s time should not be wasted with it. The public’s perception of computers was not
positive either. If motion pictures of the time can be used as a guide, the popular image was of a room-filling brain
attended by white-coated technicians, mysterious and somewhat frightening—about to eliminate jobs through automation.

Yet the machines of the early 1950s were not much more capable than Charles Babbage’s Analytical Engine of the 1830s
(although they were much faster). Although in principle these were general-purpose computers, they were still largely
restricted to doing tough math problems. They often lacked the means to perform logical operations, and they had little
text-handling capability—for example, lowercase letters were not even representable in the machines, even if there were
devices capable of printing them.

These machines could be operated only by experts, and preparing a problem for computation (what would be called
programming today) took a long time. With only one person at a time able to use a machine, major bottlenecks were
created. Problems lined up like experiments waiting for a cyclotron or the space shuttle. Much of the machine’s precious
time was wasted because of this one-at-a-time protocol.

In sum, the machines were expensive and the market was still small. To be useful in a broader business market or even in
a broader scientific market, computers would need application programs: word processors, database programs, and so on.
These applications in turn would require programming languages in which to write them and operating systems to manage
them.

Programming languages
Early computer language development
Machine language
One implication of the stored-program model was that programs could read and operate on other programs as data; that is,
they would be capable of self-modification. Konrad Zuse had looked upon this possibility as “making a contract with the
Devil” because of the potential for abuse, and he had chosen not to implement it in his machines. But self-modification
was essential for achieving a true general-purpose machine.

One of the very first employments of self-modification was for computer language translation, “language” here referring
to the instructions that make the machine work. Although the earliest machines worked by flipping switches, the
stored-program machines were driven by stored coded instructions, and the conventions for encoding these instructions
were referred to as the machine’s language.

Writing programs for early computers meant using the machine’s language. The form of a particular machine’s language is
dictated by its physical and logical structure. For example, if the machine uses registers to store intermediate results
of calculations, there must be instructions for moving data between such registers.

The vocabulary and rules of syntax of machine language tend to be highly detailed and very far from the natural or
mathematical language in which problems are normally formulated. The desirability of automating the translation of
problems into machine language was immediately evident to users, who either had to become computer experts and
programmers themselves in order to use the machines or had to rely on experts and programmers who might not fully
understand the problems they were translating.

Automatic translation from pure mathematics or some other “high-level language” to machine language was therefore
necessary before computers would be useful to a broader class of users. As early as the 1830s, Charles Babbage and Ada
Lovelace had recognized that such translation could be done by machine (see the earlier section Ada Lovelace, the first
programmer), but they made no attempt to follow up on this idea and simply wrote their programs in machine language.

Howard Aiken, working in the 1930s, also saw the virtue of automated translation from a high-level language to machine
language. Aiken proposed a coding machine that would be dedicated to this task, accepting high-level programs and
producing the actual machine-language instructions that the computer would process.

But a separate machine was not actually necessary. The IAS model guaranteed that the stored-program computer would have
the power to serve as its own coding machine. The translator program, written in machine language and running on the
computer, would be fed the target program as data, and it would output machine-language instructions. This plan was
altogether feasible, but the cost of the machines was so great that it was not seen as cost-effective to use them for
anything that a human could do—including program translation.

Two forces, in fact, argued against the early development of high-level computer languages. One was skepticism that
anyone outside the “priesthood” of computer operators could or would use computers directly. Consequently, early
computer makers saw no need to make them more accessible to people who would not use them anyway. A second reason was
efficiency. Any translation process would necessarily add to the computing time necessary to solve a problem, and
mathematicians and operators were far cheaper by the hour than computers.

Programmers did, though, come up with specialized high-level languages, or HLLs, for computer instruction—even without
automatic translators to turn their programs into machine language. They simply did the translation by hand. They did
this because casting problems in an intermediate programming language, somewhere between mathematics and the highly
detailed language of the machine, had the advantage of making it easier to understand the program’s logical structure
and to correct, or debug, any defects in the program.

The early HLLs thus were all paper-and-pencil methods of recasting problems in an intermediate form that made it easier
to write code for a machine. Herman Goldstine, with contributions from his wife, Adele Goldstine, and from John von
Neumann, created a graphical representation of this process: flow diagrams. Although the diagrams were only a notational
device, they were widely circulated and had great influence, evolving into what are known today as flowcharts.

Zuse’s Plankalkül
Konrad Zuse developed the first real programming language, Plankalkül (“Plan Calculus”), in 1944–45. Zuse’s language
allowed for the creation of procedures (also called routines or subroutines; stored chunks of code that could be invoked
repeatedly to perform routine operations such as taking a square root) and structured data (such as a record in a
database, with a mixture of alphabetic and numeric data representing, for instance, name, address, and birth date). In
addition, it provided conditional statements that could modify program execution, as well as repeat, or loop, statements
that would cause a marked block of statements or a subroutine to be repeated a specified number of times or for as long
as some condition held.

Zuse knew that computers could do more than arithmetic, but he was aware of the propensity of anyone introduced to them
to view them as nothing more than calculators. So he took pains to demonstrate nonnumeric solutions with Plankalkül. He
wrote programs to check the syntactical correctness of Boolean expressions (an application in logic and text handling)
and even to check chess moves.

Unlike flowcharts, Zuse’s program was no intermediate language intended for pencil-and-paper translation by
mathematicians. It was deliberately intended for machine translation, and Zuse did some work toward implementing a
translator for Plankalkül. He did not get very far, however; he had to disassemble his machine near the end of the war
and was not able to put it back together and work on it for several years. Unfortunately, his language and his work,
which were roughly a dozen years ahead of their time, were not generally known outside Germany.

Interpreters
HLL coding was attempted right from the start of the stored-program era in the late 1940s. Shortcode, or short-order
code, was the first such language actually implemented. Suggested by John Mauchly in 1949, it was implemented by William
Schmitt for the BINAC computer in that year and for UNIVAC in 1950. Shortcode went through multiple steps: first it
converted the alphabetic statements of the language to numeric codes, and then it translated these numeric codes into
machine language. It was an interpreter, meaning that it translated HLL statements and executed, or performed, them one
at a time—a slow process. Because of their slow execution, interpreters are now rarely used outside of program
development, where they may help a programmer to locate errors quickly.

Compilers
An alternative to this approach is what is now known as compilation. In compilation, the entire HLL program is converted
to machine language and stored for later execution. Although translation may take many hours or even days, once the
translated program is stored, it can be recalled anytime in the form of a fast-executing machine-language program.

In 1952 Heinz Rutishauser, who had worked with Zuse on his computers after the war, wrote an influential paper,
“Automatische Rechenplanfertigung bei programmgesteuerten Rechenmaschinen” (loosely translatable as “Computer Automated
Conversion of Code to Machine Language”), in which he laid down the foundations of compiler construction and described
two proposed compilers. Rutishauser was later involved in creating one of the most carefully defined programming
languages of this early era, ALGOL. (See next section, FORTRAN, COBOL, and ALGOL.)

Then, in September 1952, Alick Glennie, a student at the University of Manchester, England, created the first of several
programs called Autocode for the Manchester Mark I. Autocode was the first compiler actually to be implemented. (The
language that it compiled was called by the same name.) Glennie’s compiler had little influence, however. When J.
Halcombe Laning created a compiler for the Whirlwind computer at the Massachusetts Institute of Technology (MIT) two
years later, he met with similar lack of interest. Both compilers had the fatal drawback of producing code that ran
slower (10 times slower, in the case of Laning’s) than code handwritten in machine language.

FORTRAN, COBOL, and ALGOL
Grace Murray Hopper
While the high cost of computer resources placed a premium on fast hand-coded machine-language programs, one individual
worked tirelessly to promote high-level programming languages and their associated compilers. Grace Murray Hopper taught
mathematics at Vassar College, Poughkeepsie, New York, from 1931 to 1943 before joining the U.S. Naval Reserve. In 1944
she was assigned to the Bureau of Ordnance Computation Project at Harvard University, where she programmed the Mark I
under the direction of Howard Aiken. After World War II she joined J. Presper Eckert, Jr., and John Mauchly at their new
company and, among other things, wrote compiler software for the BINAC and UNIVAC systems. Throughout the 1950s Hopper
campaigned earnestly for high-level languages across the United States, and through her public appearances she helped to
remove resistance to the idea. Such urging found a receptive audience at IBM, where the management wanted to add
computers to the company’s successful line of business machines.

IBM develops FORTRAN
In the early 1950s John Backus convinced his managers at IBM to let him put together a team to design a language and
write a compiler for it. He had a machine in mind: the IBM 704, which had built-in floating-point math operations. That
the 704 used floating-point representation made it especially useful for scientific work, and Backus believed that a
scientifically oriented programming language would make the machine even more attractive. Still, he understood the
resistance to anything that slowed a machine down, and he set out to produce a language and a compiler that would
produce code that ran virtually as fast as hand-coded machine language—and at the same time made the program-writing
process a lot easier.

By 1954 Backus and a team of programmers had designed the language, which they called FORTRAN (Formula Translation).
Programs written in FORTRAN looked a lot more like mathematics than machine instructions:

DO 10 J = 1,11

I = 11 − J

Y = F(A(I + 1))

IF (400 − Y) 4,8,8

4 PRINT 5,1

5 FORMAT (I10, 10H TOO LARGE)

The compiler was written, and the language was released with a professional-looking typeset manual (a first for
programming languages) in 1957.

FORTRAN took another step toward making programming more accessible, allowing comments in the programs. The ability to
insert annotations, marked to be ignored by the translator program but readable by a human, meant that a well-annotated
program could be read in a certain sense by people with no programming knowledge at all. For the first time a
nonprogrammer could get an idea what a program did—or at least what it was intended to do—by reading (part of) the code.
It was an obvious but powerful step in opening up computers to a wider audience.

FORTRAN has continued to evolve, and it retains a large user base in academia and among scientists.

COBOL
About the time that Backus and his team invented FORTRAN, Hopper’s group at UNIVAC released Math-matic, a FORTRAN-like
language for UNIVAC computers. It was slower than FORTRAN and not particularly successful. Another language developed at
Hopper’s laboratory at the same time had more influence. Flow-matic used a more English-like syntax and vocabulary:

1 COMPARE PART-NUMBER (A) TO PART-NUMBER (B);

IF GREATER GO TO OPERATION 13;

IF EQUAL GO TO OPERATION 4;

OTHERWISE GO TO OPERATION 2.

Flow-matic led to the development by Hopper’s group of COBOL (Common Business-Oriented Language) in 1959. COBOL was
explicitly a business programming language with a very verbose English-like style. It became central to the wide
acceptance of computers by business after 1959.

ALGOL
Although both FORTRAN and COBOL were universal languages (meaning that they could, in principle, be used to solve any
problem that a computer could unravel), FORTRAN was better suited for mathematicians and engineers, whereas COBOL was
explicitly a business programming language.

During the late 1950s a multitude of programming languages appeared. This proliferation of incompatible specialized
languages spurred an interest in the United States and Europe to create a single “second-generation” language. A
transatlantic committee soon formed to determine specifications for ALGOL (Algorithmic Language), as the new language
would be called. Backus, on the American side, and Heinz Rutishauser, on the European side, were among the most
influential committee members.

Although ALGOL introduced some important language ideas, it was not a commercial success. Customers preferred a known
specialized language, such as FORTRAN or COBOL, to an unknown general-programming language. Only Pascal, a scientific
programming-language offshoot of ALGOL, survives.

Operating systems
Control programs
In order to make the early computers truly useful and efficient, two major innovations in software were needed. One was
high-level programming languages (as described in the preceding section, FORTRAN, COBOL, and ALGOL). The other was
control. Today the systemwide control functions of a computer are generally subsumed under the term operating system, or
OS. An OS handles the behind-the-scenes activities of a computer, such as orchestrating the transitions from one program
to another and managing access to disk storage and peripheral devices.

The need for some kind of supervisor program was quickly recognized, but the design requirements for such a program were
daunting. The supervisor program would have to run in parallel with an application program somehow, monitor its actions
in some way, and seize control when necessary. Moreover, the essential—and difficult—feature of even a rudimentary
supervisor program was the interrupt facility. It had to be able to stop a running program when necessary but save the
state of the program and all registers so that after the interruption was over the program could be restarted from where
it left off.

The first computer with such a true interrupt system was the UNIVAC 1103A, which had a single interrupt triggered by one
fixed condition. In 1959 the Lincoln Labs TX2 generalized the interrupt capability, making it possible to set various
interrupt conditions under software control. However, it would be one company, IBM, that would create, and dominate, a
market for business computers. IBM established its primacy primarily through one invention: the IBM 360 operating
system.

The IBM 360
IBM had been selling business machines since early in the century and had built Howard Aiken’s computer to his
architectural specifications. But the company had been slow to implement the stored-program digital computer
architecture of the early 1950s. It did develop the IBM 650, a (like UNIVAC) decimal implementation of the IAS plan—and
the first computer to sell more than 1,000 units.

The invention of the transistor in 1947 led IBM to reengineer its early machines from electromechanical or vacuum tube
to transistor technology in the late 1950s (although the UNIVAC Model 80, delivered in 1958, was the first transistor
computer). These transistorized machines are commonly referred to as second-generation computers.

Two IBM inventions, the magnetic disk and the high-speed chain printer, led to an expansion of the market and to the
unprecedented sale of 12,000 computers of one model: the IBM 1401. The chain printer required a lot of magnetic core
memory, and IBM engineers packaged the printer support, core memory, and disk support into the 1401, one of the first
computers to use this solid-state technology.

IBM had several lines of computers developed by independent groups of engineers within the company: a
scientific-technical line, a commercial data-processing line, an accounting line, a decimal machine line, and a line of
supercomputers. Each line had a distinct hardware-dependent operating system, and each required separate development and
maintenance of its associated application software. In the early 1960s IBM began designing a machine that would take the
best of all these disparate lines, add some new technology and new ideas, and replace all the company’s computers with
one single line, the 360. At an estimated development cost of $5 billion, IBM literally bet the company’s future on this
new, untested architecture.

The 360 was in fact an architecture, not a single machine. Designers G.M. Amdahl, F.P. Brooks, and G.A. Blaauw
explicitly separated the 360 architecture from its implementation details. The 360 architecture was intended to span a
wide range of machine implementations and multiple generations of machines. The first 360 models were hybrid
transistor–integrated circuit machines. Integrated circuit computers are commonly referred to as third-generation
computers.

Key to the architecture was the operating system. OS/360 ran on all machines built to the 360 architecture—initially six
machines spanning a wide range of performance characteristics and later many more machines. It had a shielded
supervisory system (unlike the 1401, which could be interfered with by application programs), and it reserved certain
operations as privileged in that they could be performed only by the supervisor program.

The first IBM 360 computers were delivered in 1965. The 360 architecture represented a continental divide in the
relative importance of hardware and software. After the 360, computers were defined by their operating systems.

How the Elliott 803 transistorized computer works1 of 2
How the Elliott 803 transistorized computer worksLearn about the Elliott 803, a transistorized computer made in the
United Kingdom in the early 1960s.
See all videos for this article
What is the ICL 2966 computer and how does it work?2 of 2
What is the ICL 2966 computer and how does it work?Learn about the ICL 2966, a mainframe computer utilizing integrated
circuit technology, made in the United Kingdom in the 1980s.
See all videos for this article
The market, on the other hand, was defined by IBM. In the late 1950s and into the 1960s, it was common to refer to the
computer industry as “IBM and the Seven Dwarfs,” a reference to the relatively diminutive market share of its nearest
rivals—Sperry Rand (UNIVAC), Control Data Corporation (CDC), Honeywell, Burroughs, General Electric (GE), RCA, and
National Cash Register Co. During this time IBM had some 60–70 percent of all computer sales. The 360 did nothing to
lessen the giant’s dominance. When the market did open up somewhat, it was not due to the efforts of, nor was it in
favor of, the dwarfs. Yet, while “IBM and the Seven Dwarfs” (soon reduced to “IBM and the BUNCH of Five,” BUNCH being an
acronym for Burroughs, UNIVAC, NCR, CDC, and Honeywell) continued to build Big Iron, a fundamental change was taking
place in how computers were accessed.

Time-sharing and minicomputers
Time-sharing from Project MAC to UNIX
In 1959 Christopher Strachey in the United Kingdom and John McCarthy in the United States independently described
something they called time-sharing. Meanwhile, computer pioneer J.C.R. Licklider at the Massachusetts Institute of
Technology (MIT) began to promote the idea of interactive computing as an alternative to batch processing. Batch
processing was the normal mode of operating computers at the time: a user handed a deck of punched cards to an operator,
who fed them to the machine, and an hour or more later the printed output would be made available for pickup.
Licklider’s notion of interactive programming involved typing on a teletype or other keyboard and getting more or less
immediate feedback from the computer on the teletype’s printer mechanism or some other output device. This was how the
Whirlwind computer had been operated at MIT in 1950, and it was essentially what Strachey and McCarthy had in mind at
the end of the decade.

By November 1961 a prototype time-sharing system had been produced and tested. It was built by Fernando Corbato and
Robert Jano at MIT, and it connected an IBM 709 computer with three users typing away at IBM Flexowriters. This was only
a prototype for a more elaborate time-sharing system that Corbato was working on, called Compatible Time-Sharing System,
or CTSS. Still, Corbato was waiting for the appropriate technology to build that system. It was clear that
electromechanical and vacuum tube technologies would not be adequate for the computational demands that time-sharing
would place on the machines. Fast, transistor-based computers were needed.

In the meantime, Licklider had been placed in charge of a U.S. government program called the Advanced Research Projects
Agency (ARPA), created in response to the launch of the Sputnik satellite by the Soviet Union in 1957. ARPA researched
interesting technological areas, and under Licklider’s leadership it focused on time-sharing and interactive computing.
With ARPA support, CTSS evolved into Project MAC, which went online in 1963.

Project MAC was only the beginning. Other similar time-sharing projects followed rapidly at various research
institutions, and some commercial products began to be released that also were called interactive or time-sharing. (The
role of ARPA in creating another time-sharing network, ARPANET, became the foundation of the Internet and is discussed
in a later section, The Internet.)

Time-sharing represented a different interaction model, and it needed a new programming language to support it.
Researchers created several such languages, most notably BASIC (Beginner’s All-Purpose Symbolic Instruction Code), which
was invented in 1964 at Dartmouth College, Hanover, New Hampshire, by John Kemeny and Thomas Kurtz. BASIC had features
that made it ideal for time-sharing, and it was easy enough to be used by its target audience: college students. Kemeny
and Kurtz wanted to open computers to a broader group of users and deliberately designed BASIC with that goal in mind.
They succeeded.

Time-sharing also called for a new kind of operating system. Researchers at AT&T (American Telephone and Telegraph
Company) and GE tackled the problem with funding from ARPA via Project MAC and an ambitious plan to implement
time-sharing on a new computer with a new time-sharing-oriented operating system. AT&T dropped out after the project was
well under way, but GE went ahead, and the result was the Multics operating system running on the GE 645 computer. GE
645 exemplified the time-shared computer in 1965, and Multics was the model of a time-sharing operating system, built to
be up seven days a week, 24 hours a day.

When AT&T dropped out of the project and removed the GE machines from its laboratories, researchers at AT&T’s high-tech
research arm, Bell Laboratories, were upset. They felt they needed the time-sharing capabilities of Multics for their
work, and so two Bell Labs workers, Ken Thompson and Dennis Ritchie, wrote their own operating system. Since the
operating system was inspired by Multics but would initially be somewhat simpler, they called it UNIX.

UNIX embodied, among other innovations, the notion of pipes. Pipes allowed a user to pass the results of one program to
another program for use as input. This led to a style of programming in which small, targeted, single-function programs
were joined together to achieve a more complicated goal. Perhaps the most influential aspect of UNIX, though, was that
Bell Labs distributed the source code (the uncompiled, human-readable form of the code that made up the operating
system) freely to colleges and universities—but made no offer to support it. The freely distributed source code led to a
rapid, and somewhat divergent, evolution of UNIX. Whereas initial support was attracted by its free availability, its
robust multitasking and well-developed network security features have continued to make it the most common operating
system for academic institutions and World Wide Web servers.

Minicomputers
About 1965, roughly coterminous with the development of time-sharing, a new kind of computer came on the scene. Small
and relatively inexpensive (typically one-tenth the cost of the Big Iron machines), the new machines were stored-program
computers with all the generality of the computers then in use but stripped down. The new machines were called
minicomputers. (About the same time, the larger traditional computers began to be called mainframes.) Minicomputers were
designed for easy connection to scientific instruments and other input/output devices, had a simplified architecture,
were implemented using fast transistors, and were typically programmed in assembly language with little support for
high-level languages.

Other small, inexpensive computing devices were available at the time but were not considered minicomputers. These were
special-purpose scientific machines or small character-based or decimal-based machines such as the IBM 1401. They were
not considered “minis,” however, because they did not meet the needs of the initial market for minis—that is, for a lab
computer to control instruments and collect and analyze data.

The market for minicomputers evolved over time, but it was scientific laboratories that created the category. It was an
essentially untapped market, and those manufacturers who established an early foothold dominated it. Only one of the
mainframe manufacturers, Honeywell, was able to break into the minicomputer market in any significant way. The other
main minicomputer players, such as Digital Equipment Corporation (DEC), Data General Corporation, Hewlett-Packard
Company, and Texas Instruments Incorporated, all came from fields outside mainframe computing, frequently from the field
of electronic test equipment. The failure of the mainframe companies to gain a foothold in the minimarket may have
stemmed from their failure to recognize that minis were distinct in important ways from the small computers that these
companies were already making.

The first minicomputer, although it was not recognized as such at the time, may have been the MIT Whirlwind in 1950. It
was designed for instrument control and had many, although not all, of the features of later minis. DEC, founded in 1957
by Kenneth Olsen and Harlan Anderson, produced one of the first minicomputers, the Programmed Data Processor, or PDP-1,
in 1959. At a price of $120,000, the PDP-1 sold for a fraction of the cost of mainframe computers, albeit with vastly
more limited capabilities. But it was the PDP-8, using the recently invented integrated circuit (a set of interconnected
transistors and resistors on a single silicon wafer, or chip) and selling for around $20,000 (falling to $3,000 by the
late 1970s), that was the first true mass-market minicomputer. The PDP-8 was released in 1965, the same year as the
first IBM 360 machines.

The PDP-8 was the prototypical mini. It was designed to be programmed in assembly language; it was easy—physically,
logically, and electrically—to attach a wide variety of input/output devices and scientific instruments to it; and it
was architecturally stripped down with little support for programming—it even lacked multiplication and division
operations in its initial release. It had a mere 4,096 words of memory, and its word length was 12 bits—very short even
by the standards of the times. (The word is the smallest chunk of memory that a program can refer to independently; the
size of the word limits the complexity of the instruction set and the efficiency of mathematical operations.) The
PDP-8’s short word and small memory made it relatively underpowered for the time, but its low price more than
compensated for this.

The PDP-11 shipped five years later, relaxing some of the constraints imposed on the PDP-8. It was designed to support
high-level languages, had more memory and more power generally, was produced in 10 different models over 10 years, and
was a great success. It was followed by the VAX line, which supported an advanced operating system called VAX/VMS—VMS
standing for virtual memory system, an innovation that effectively expanded the memory of the machine by allowing disk
or other peripheral storage to serve as extra memory. By this time (the early 1970s) DEC was vying with Sperry Rand
(manufacturer of the UNIVAC computer) for position as the second largest computer company in the world, though it was
producing machines that had little in common with the original prototypical minis.

Although the minis’ early growth was due to their use as scientific instrument controllers and data loggers, their
compelling feature turned out to be their approachability. After years of standing in line to use departmental,
university-wide, or company-wide machines through intermediaries, scientists and researchers could now buy their own
computer and run it themselves in their own laboratories. And they had intimate access to the internals of the machine,
the stripped-down architecture making it possible for a smart graduate student to reconfigure the machine to do
something not intended by the manufacturer. With their own computers in their labs, researchers began to use minis for
all sorts of new purposes, and the manufacturers adapted later releases of the machines to the evolving demands of the
market.

The minicomputer revolution lasted about a decade. By 1975 it was coming to a close, but not because minis were becoming
less attractive. The mini was about to be eclipsed by another technology: the new integrated circuits, which would soon
be used to build the smallest, most affordable computers to date. The rise of this new technology is described in the
next section, The personal computer revolution.

The personal computer revolution
Before 1970, computers were big machines requiring thousands of separate transistors. They were operated by specialized
technicians, who often dressed in white lab coats and were commonly referred to as a computer priesthood. The machines
were expensive and difficult to use. Few people came in direct contact with them, not even their programmers. The
typical interaction was as follows: a programmer coded instructions and data on preformatted paper, a keypunch operator
transferred the data onto punch cards, a computer operator fed the cards into a card reader, and the computer executed
the instructions or stored the cards’ information for later processing. Advanced installations might allow users limited
interaction with the computer more directly, but still remotely, via time-sharing through the use of cathode-ray tube
terminals or teletype machines.

At the beginning of the 1970s there were essentially two types of computers. There were room-sized mainframes, costing
hundreds of thousands of dollars, that were built one at a time by companies such as IBM and CDC. There also were
smaller, cheaper, mass-produced minicomputers, costing tens of thousands of dollars, that were built by a handful of
companies, such as Digital Equipment Corporation and Hewlett-Packard Company, for scientific laboratories and
businesses.

Still, most people had no direct contact with either type of computer, and the machines were popularly viewed as
impersonal giant brains that threatened to eliminate jobs through automation. The idea that anyone would have his or her
own desktop computer was generally regarded as far-fetched. Nevertheless, with advances in integrated circuit
technology, the necessary building blocks for desktop computing began to emerge in the early 1970s.

The microprocessor
Integrated circuits
William Shockley, a co-inventor of the transistor, started Shockley Semiconductor Laboratories in 1955 in his hometown
of Palo Alto, California. In 1957 his eight top researchers left to form Fairchild Semiconductor Corporation, funded by
Fairchild Camera and Instrument Corporation. Along with Hewlett-Packard, another Palo Alto firm, Fairchild Semiconductor
was the seed of what would become known as Silicon Valley. Historically, Fairchild will always deserve recognition as
one of the most important semiconductor companies, having served as the training ground for most of the entrepreneurs
who went on to start their own computer companies in the 1960s and early 1970s.

From the mid-1960s into the early ’70s, Fairchild Semiconductor Corporation and Texas Instruments Incorporated were the
leading manufacturers of integrated circuits (ICs) and were continually increasing the number of electronic components
embedded in a single silicon wafer, or chip. As the number of components escalated into the thousands, these chips began
to be referred to as large-scale integration chips, and computers using them are sometimes called fourth-generation
computers. The invention of the microprocessor was the culmination of this trend.

Although computers were still rare and often regarded as a threat to employment, calculators were common and accepted in
offices. With advances in semiconductor technology, a market was emerging for sophisticated electronic desktop
calculators. It was, in fact, a calculator project that turned into a milestone in the history of computer technology.

The Intel 4004
In 1969 Busicom, a Japanese calculator company, commissioned Intel Corporation to make the chips for a line of
calculators that Busicom intended to sell. Custom chips were made for many clients, and this was one more such contract,
hardly unusual at the time.

Intel was one of several semiconductor companies to emerge in Silicon Valley, having spun off from Fairchild
Semiconductor. Intel’s president, Robert Noyce, while at Fairchild, had invented planar integrated circuits, a process
in which the wiring was directly embedded in the silicon along with the electronic components at the manufacturing
stage.

Intel had planned on focusing its business on memory chips, but Busicom’s request for custom chips for a calculator
turned out to be a most valuable diversion. While specialized chips were effective at their given task, their small
market made them expensive. Three Intel engineers—Federico Faggin, Marcian (“Ted”) Hoff, and Stan Mazor—considered the
request of the Japanese firm and proposed a more versatile design.

Hoff had experience with minicomputers, which could do anything the calculator could do and more. He rebelled at
building a special-purpose device when the technology existed to build a general-purpose one. The general-purpose device
he had in mind, however, would be a lot like a computer, and at that time computers intimidated people while calculators
did not. Moreover, there was a clear and large market for calculators and a limited one for computers—and, after all,
the customer had commissioned a calculator chip.

Nevertheless, Hoff prevailed, and Intel proposed a design that was functionally very similar to a minicomputer (although
not in size, power, attachable physical devices such as printers, or many other practical ways). In addition to
performing the input/output functions that most ICs carried out, the design would form the instructions for the IC and
would help to control, send, and receive signals from other chips and devices. A set of instructions was stored in
memory, and the chip could read them and respond to them. The device would thus do everything that Busicom wanted, but
it would do a lot more: it was the essence of a general-purpose computer. There was little obvious demand for such a
device, but the Intel team, understanding the drawbacks of special-purpose ICs, sensed that it was an economical device
that would, somehow, find a market.

At first Busicom was not interested, but Intel decided to go forward with the design anyway, and the Japanese company
eventually accepted it. Intel named the chip the 4004, which referred to the number of features and transistors it had.
These included memory, input/output, control, and arithmetical/logical capacities. It came to be called a microprocessor
or microcomputer. It is this chip that is referred to as the brain of the personal desktop computer—the central
processing unit, or CPU.

Busicom eventually sold over 100,000 calculators powered by the 4004. Busicom later also accepted a one-time payment of
$60,000 that gave Intel exclusive rights to the 4004 design, and Intel began marketing the chip to other manufacturers
in 1971.

The 4004 had significant limitations. As a four-bit processor, it was capable of only 24, or 16, distinct combinations,
or “words.” To distinguish the 26 letters of the alphabet and up to six punctuation symbols, the computer had to combine
two four-bit words. Nevertheless, the 4004 achieved a level of fame when Intel found a high-profile customer for it: it
was used on the Pioneer 10 space probe, launched on March 2, 1972.

It became a little easier to see the potential of microprocessors when Intel introduced an eight-bit processor, the
8008, in November 1972. (In 1974 the 8008 was reengineered with a larger, more versatile instruction set as the 8080.)
In 1972 Intel was still a small company, albeit with two new and revolutionary products. But no one—certainly not their
inventors—had figured out exactly what to do with Intel’s microprocessors.

Intel placed in electronics magazines articles expounding the microprocessors’ capabilities and proselytized engineering
organizations and companies in the hope that others would come up with applications. With the basic capabilities of a
computer now available on a tiny speck of silicon, some observers realized that this was the dawn of a new age of
computing. That new age would center on the microcomputer.

The microcomputer
Early computer enthusiasts
Though the young engineering executives at Intel could sense the ground shifting upon the introduction of their new
microprocessors, the leading computer manufacturers did not. It should not have taken a visionary to observe the trend
of cheaper, faster, and more powerful devices. Nevertheless, even after the invention of the microprocessor, few could
imagine a market for personal computers.

The advent of the microprocessor did not inspire IBM or any other large company to begin producing personal computers.
Time after time, the big computer companies overlooked the opportunity to bring computing capabilities to a much broader
market. In some cases, they turned down explicit proposals by their own engineers to build such machines. Instead, the
new generation of microcomputers or personal computers emerged from the minds and passions of electronics hobbyists and
entrepreneurs.

In the San Francisco Bay area, the advances of the semiconductor industry were gaining recognition and stimulating a
grassroots computer movement. Lee Felsenstein, an electronics engineer active in the student antiwar movement of the
1960s, started an organization called Community Memory to install computer terminals in storefronts. This movement was a
sign of the times, an attempt by computer cognoscenti to empower the masses by giving ordinary individuals access to a
public computer network.

The frustration felt by engineers and electronics hobbyists who wanted easier access to computers was expressed in
articles in the electronics magazines in the early 1970s. Magazines such as Popular Electronics and Radio Electronics
helped spread the notion of a personal computer. And in the San Francisco Bay area and elsewhere hobbyists organized
computer clubs to discuss how to build their own computers.

Dennis Allison wrote a version of BASIC for these early personal computers and, with Bob Albrecht, published the code in
1975 in a newsletter called Dr. Dobb’s Journal of Computer Calisthenics and Orthodontia, later changed to Dr. Dobb’s
Journal.

The Altair
In September 1973 Radio Electronics published an article describing a “TV Typewriter,” which was a computer terminal
that could connect a hobbyist with a mainframe computer. It was written by Don Lancaster, an aerospace engineer and fire
spotter in Arizona who was also a prolific author of do-it-yourself articles for electronics hobbyists. The TV
Typewriter provided the first display of alphanumeric information on a common television set. It influenced a generation
of computer hobbyists to start thinking about real “home-brewed” computers.

The next step was the personal computer itself. That same year a French company, R2E, developed the Micral microcomputer
using the 8008 processor. The Micral was the first commercial, non-kit microcomputer. Although the company sold 500
Micrals in France that year, it was little known among American hobbyists.

Instead, a company called Micro Instrumentation Telemetry Systems, which rapidly became known as MITS, made the big
American splash. This company, located in a tiny office in an Albuquerque, New Mexico, shopping center, had started out
selling radio transmitters for model airplanes in 1968. It expanded into the kit calculator business in the early 1970s.
This move was terribly ill-timed because other, larger manufacturers such as Hewlett-Packard and Texas Instruments
(itself a leading designer of ICs) soon moved into the market with mass-produced calculators. As a result, calculators
quickly became smaller, more powerful, and cheaper. By 1974 the average cost for a calculator had dropped from several
hundred dollars to about $25, and MITS was on the verge of bankruptcy.

In need of a new product, MITS came up with the idea of selling a computer kit. The kit, containing all of the
components necessary to build an Altair computer, sold for $397, barely more than the list cost of the Intel 8080
microprocessor that it used. A January 1975 cover article in Popular Electronics generated hundreds of orders for the
kit, and MITS was saved.

The firm did its best to live up to its promise of delivery within 60 days, and to do so it limited manufacture to a
bare-bones kit that included a box, a CPU board with 256 bytes of memory, and a front panel. The machines, especially
the early ones, had only limited reliability. To make them work required many hours of assembly by an electronics
expert.

When assembled, Altairs were blue, box-shaped machines that measured approximately 43 cm by 46 cm by 18 cm (17 inches by
18 inches by 7 inches). There was no keyboard, video terminal, paper-tape reader, or printer. There was no software. All
programming was in assembly language. The only way to input programs was by setting switches on the front panel for each
instruction, step-by-step. A pattern of flashing lights on the front panel indicated the results of a program.

Just getting the Altair to blink its lights represented an accomplishment. Nevertheless, it sparked people’s interest.
In Silicon Valley, members of a nascent hobbyist group called the Homebrew Computer Club gathered around an Altair at
one of their first meetings. Homebrew epitomized the passion and antiestablishment camaraderie that characterized the
hobbyist community in Silicon Valley. At their meetings, chaired by Felsenstein, attendees compared digital devices that
they were constructing and discussed the latest articles in electronics magazines.

In one important way, MITS modeled the Altair after the minicomputer. It had a bus structure, a data path for sending
instructions throughout its circuitry that would allow it to house and communicate with add-on circuit boards. The
Altair hardly represented a singular revolutionary invention, along the lines of the transistor, but it did encourage
sweeping change, giving hobbyists the confidence to take the next step.

The hobby market expands
Some entrepreneurs, particularly in the San Francisco Bay area, saw opportunities to build add-on devices, or
peripherals, for the Altair; others decided to design competitive hardware products. Because different machines might
use different data paths, or buses, peripherals built for one computer might not work with another computer. This led
the emerging industry to petition the Institute for Electrical and Electronics Engineers to select a standard bus. The
resulting standard, the S-100 bus, was open for all to use and became ubiquitous among early personal computers.
Standardizing on a common bus helped to expand the market for early peripheral manufacturers, spurred the development of
new devices, and relieved computer manufacturers of the onerous need to develop their own proprietary peripherals.

These early microcomputer companies took the first steps toward building a personal computer industry, but most of them
eventually collapsed, unable to build enough reliable machines or to offer sufficient customer support. In general, most
of the early companies lacked the proper balance of engineers, entrepreneurs, capital, and marketing experience. But
perhaps even more significant was a dearth of software that could make personal computers useful to a larger,
nonhobbyist market.

Early microcomputer software
From Star Trek to Microsoft
The first programs developed for the hobbyists’ microcomputers were games. With the early machines limited in graphic
capabilities, most of these were text-based adventure or role-playing games. However, there were a few graphical games,
such as Star Trek, which were popular on mainframes and minicomputers and were converted to run on microcomputers. One
company created the game Micro Chess and used the profits to fund the development of an important program called
VisiCalc, the industry’s first spreadsheet software. These games, in addition to demonstrating some of the
microcomputer’s capabilities, helped to convince ordinary individuals, in particular small-business owners, that they
could operate a computer.

As was the case with large computers, the creation of application software for the machines waited for the development
of programming languages and operating systems. Gary Kildall developed the first operating system for a microcomputer as
part of a project he contracted with Intel several years before the release of the Altair. Kildall realized that a
computer had to be able to handle storage devices such as disk drives, and for this purpose he developed an operating
system called CP/M.

There was no obvious use for such software at the time, and Intel agreed that Kildall could keep it. Later, when a few
microcomputer companies had emerged from among the hobbyists and entrepreneurs inspired by MITS, a company called IMSAI
realized that an operating system would attract more software to its machine, and it chose CP/M. Most companies followed
suit, and Kildall’s company, Digital Research, became one of the first software giants in the emerging microcomputer
industry.

High-level languages were also needed in order for programmers to develop applications. Two young programmers realized
this almost immediately upon hearing of the MITS Altair. Childhood friends Bill Gates and Paul Allen were whiz kids with
computers as they grew up in Seattle, Washington, debugging software on minicomputers at the ages of 13 and 15,
respectively. As teenagers they had started a company and had built the hardware and written the software that would
provide statistics on traffic flow from a rubber tube strung across a highway. Later, when the Altair came out, Allen
quit his job, and Gates left Harvard University, where he was a student, in order to create a version of the programming
language BASIC that could run on the new computer. They licensed their version of BASIC to MITS and started calling
their partnership Microsoft. The Microsoft Corporation went on to develop versions of BASIC for nearly every computer
that was released. It also developed other high-level languages. When IBM eventually decided to enter the microcomputer
business in 1980, it called on Microsoft for both a programming language and an operating system, and the small
partnership was on its way to becoming the largest software company in the world. (See the section The IBM Personal
Computer.)

Application software
The availability of BASIC and CP/M enabled more widespread software development. By 1977 a two-person firm called
Structured Systems Group started developing a General Ledger program, perhaps the first serious business software, which
sold for $995. The company shipped its software in ziplock bags with a manual, a practice that became common in the
industry. General Ledger began to familiarize business managers with microcomputers. Another important program was the
first microcomputer word processor, called Electric Pencil, developed by a former camera operator turned computer
hobbyist. Electric Pencil was one of the first programs that allowed nontechnical people to perform useful tasks on
personal computers. Nevertheless, the early personal computer companies still underestimated the value of software, and
many refused to pay the software developer to convert Electric Pencil to run on their machines. Eventually the
availability of some software would play a major role in determining the success of a computer.

In 1979 a Harvard business graduate named Dan Bricklin and a programmer named Bob Frankston developed VisiCalc, the
first personal computer financial analysis tool. VisiCalc made business forecasting much simpler, allowing individuals
to ask “What if” questions about numerical data and get the sort of immediate response that was not even possible for
giant corporations using mainframe computer systems. Personal Software, the company that distributed VisiCalc, became
hugely successful. With a few companies such as Microsoft leading the way, a software industry separate from the
hardware field began to emerge.

The personal computer
Commodore and Tandy enter the field
In late 1976 Commodore Business Machines, an established electronics firm that had been active in producing electronic
calculators, bought a small hobby-computer company named MOS Technology. For the first time, an established company with
extensive distribution channels would be selling a microcomputer.

Acorn Computers' BBC Micro explained
Acorn Computers' BBC Micro explainedLearn about the BBC Micro, a microcomputer made in the 1980s by Acorn Computers, a
British company.
See all videos for this article
The next year, another established company entered the microcomputer market. Tandy Corporation, best known for its chain
of Radio Shack stores, had followed the development of MITS and decided to enter the market with its own TRS-80
microcomputer, which came with four kilobytes of memory, a Z80 microprocessor, a BASIC programming language, and
cassettes for data storage. To cut costs, the machine was built without the ability to type lowercase letters. Thanks to
Tandy’s chain of stores and the breakthrough price ($399 fully assembled and tested), the machine was successful enough
to convince the company to introduce a more powerful computer two years later, the TRS-80 Model II, which could
reasonably be marketed as a small-business computer. Tandy started selling its computers in greater volumes than most of
the microcomputer start-ups, except for one.

Apple Inc.
Apple I
Apple I Steve Jobs (right) and Steve Wozniak holding an Apple I circuit board, c. 1976.
Like the founding of the early chip companies and the invention of the microprocessor, the story of Apple is a key part
of Silicon Valley folklore. Two whiz kids, Steve Wozniak and Steve Jobs, shared an interest in electronics. Wozniak was
an early and regular participant at Homebrew Computer Club meetings (see the earlier section, The Altair), which Jobs
also occasionally attended.

Wozniak purchased one of the early microprocessors, the Mostek 6502 (made by MOS Technology), and used it to design a
computer. When Hewlett-Packard, where he had an internship, declined to build his design, he shared his progress at a
Homebrew meeting, where Jobs suggested that they could sell it together. Their initial plans were modest. Jobs figured
that they could sell it for $50, twice what the parts cost them, and that they could sell hundreds of them to hobbyists.
The product was actually only a printed circuit board. It lacked a case, a keyboard, and a power supply. Jobs got an
order for 50 of the machines from Paul Terrell, owner of one of the industry’s first computer retail stores and a
frequent Homebrew attendee. To raise the capital to buy the parts they needed, Jobs sold his minibus and Wozniak his
calculator. They met their 30-day deadline and continued production in Jobs’s parents’ garage.

After their initial success, Jobs sought out the kind of help that other industry pioneers had shunned. While he and
Wozniak began work on the Apple II, he consulted with a venture capitalist and enlisted an advertising company to aid
him in marketing. As a result, in late 1976 A.C. (“Mike”) Markkula, a retired semiconductor company executive, helped
write a business plan for Apple, lined up credit from a bank, and hired a serious businessman to run the venture. Apple
was clearly taking a different path from its competitors. For instance, while Altair and the other microcomputer
start-ups ran advertisements in technical journals, Apple ran an early color ad in Playboy magazine. Its executive team
lined up nationwide distributors. Apple made sure each of its subsequent products featured an elegant, consumer-style
design. It also published well-written and carefully designed manuals to instruct consumers on the use of the machines.
Other manuals explained all the technical details any third-party hardware or software company would have to know to
build peripherals. In addition, Apple quickly built well-engineered products that made the Apple II far more useful: a
printer card, a serial card, a communications card, a memory card, and a floppy disk. This distinctive approach
resonated well in the marketplace.

In 1980 the Apple III was introduced. For this new computer Apple designed a new operating system, though it also
offered a capability known as emulation that allowed the machine to run the same software, albeit much slower, as the
Apple II. After several months on the market the Apple III was recalled so that certain defects could be repaired
(proving that Apple was not immune to the technical failures from which most early firms suffered), but upon
reintroduction to the marketplace it never achieved the success of its predecessor (demonstrating how difficult it can
be for a company to introduce a computer that is not completely compatible with its existing product line).

Nevertheless, the flagship Apple II and successors in that line—the Apple II+, the Apple IIe, and the Apple IIc—made
Apple into the leading personal computer company in the world. In 1980 it announced its first public stock offering, and
its young founders became instant millionaires. After three years in business, Apple’s revenues had increased from $7.8
million to $117.9 million.

The graphical user interface
graphical user interface
graphical user interfaceThe Xerox Alto was the first computer to use graphical icons and a mouse to control the
system—the first graphical user interface (GUI).
In 1982 Apple introduced its Lisa computer, a much more powerful computer with many innovations. The Lisa used a more
advanced microprocessor, the Motorola 68000. It also had a different way of interacting with the user, called a
graphical user interface (GUI). The GUI replaced the typed command lines common on previous computers with graphical
icons on the screen that invoked actions when pointed to by a handheld pointing device called the mouse. The Lisa was
not successful, but Apple was already preparing a scaled-down, lower-cost version called the Macintosh. Introduced in
1984, the Macintosh became wildly successful and, by making desktop computers easier to use, further popularized
personal computers.

first computer mouse
first computer mouse Douglas Engelbart invented the computer mouse in 1963–64 as part of an experiment to find a better
way to point and click on a display screen. Fashioned at the Stanford Research Institute, it had a carved wood casing
and just one button. A subsequent model had three buttons, and Engelbart would have provided more if there had been room
for more than the three microswitches to which the buttons were connected.
The Lisa and the Macintosh popularized several ideas that originated at other research laboratories in Silicon Valley
and elsewhere. These underlying intellectual ideas, centered on the potential impact that computers could have on
people, had been nurtured first by Vannevar Bush in the 1940s and then by Douglas Engelbart. Like Bush, who inspired
him, Engelbart was a visionary. As early as 1963 he was predicting that the computer would eventually become a tool to
augment human intellect, and he specifically described many of the uses computers would have, such as word processing.
In 1968, as a researcher at the Stanford Research Institute (SRI), Engelbart gave a remarkable demonstration of the
“NLS” (oNLine System), which featured a keyboard and a mouse, a device he had invented that was used to select commands
from a menu of choices shown on a display screen. The screen was divided into multiple windows, each able to display
text—a single line or an entire document—or an image. Today almost every popular computer comes with a mouse and
features a system that utilizes windows on the display.

In the 1970s some of Engelbart’s colleagues left SRI for Xerox Corporation’s Palo Alto (California) Research Center
(PARC), which became a hotbed of computer research. In the coming years scientists at PARC pioneered many new
technologies. Xerox built a prototype computer with a GUI operating system called the Alto and eventually introduced a
commercial version called the Xerox Star in 1981. Xerox’s efforts to market this computer were a failure, and the
company withdrew from the market. Apple with its Lisa and Macintosh computers and then Microsoft with its Windows
operating system imitated the design of the Alto and Star systems in many ways.

Two computer scientists at PARC, Alan Kay and Adele Goldberg, published a paper in the early 1970s describing a vision
of a powerful and portable computer they dubbed the Dynabook. The prototypes of this machine were expensive and
resembled sewing machines, but the vision of the two researchers greatly influenced the evolution of products that today
are dubbed notebook or laptop computers.

Another researcher at PARC, Robert Metcalfe, developed a network system in 1973 that could transmit and receive data at
three million bits a second, much faster than was generally thought possible at the time. Xerox did not see this as
related to its core business of copiers, and it allowed Metcalfe to start his own company based on the system, called
Ethernet. Ethernet eventually became the technical standard for connecting digital computers together in an office
environment.

PARC researchers used Ethernet to connect their Altos together and to share another invention of theirs, the laser
printer. Laser printers work by shooting a stream of light that gives a charge to the surface of a rotating drum. The
charged area attracts toner powder so that when paper rolls over it an image is transferred. PARC programmers also
developed numerous other innovations, such as the Smalltalk programming language, designed to make programming
accessible to users who were not computer experts, and a text editor called Bravo, which displayed text on a computer
screen exactly as it would look on paper.

Xerox PARC came up with these innovations but left it to others to commercialize them. Today they are viewed as
commonplace.

The IBM Personal Computer
IBM Personal Computer
IBM Personal ComputerThe IBM Personal Computer (PC) was introduced in 1981. Microsoft supplied the machine's operating
system, MS-DOS (Microsoft Disk Operating System).
The entry of IBM did more to legitimize personal computers than any event in the industry’s history. By 1980 the
personal computer field was starting to interest the large computer companies. Hewlett-Packard, which had earlier turned
down Steve Wozniak’s proposal to enter the personal computer field, was now ready to enter this business, and in January
1980 it brought out its HP-85. Hewlett-Packard’s machine was more expensive ($3,250) than those of most competitors, and
it used a cassette tape drive for storage while most companies were already using disk drives. Another problem was its
closed architecture, which made it difficult for third parties to develop applications or software for it.

Throughout its history IBM had shown a willingness to place bets on new technologies, such as the 360 architecture. (See
the earlier section The IBM 360.) Its long-term success was due largely to its ability to innovate and to adapt its
business to technological change. “Big Blue,” as the company was commonly known, introduced the first computer disk
storage system, the RAMAC, which showed off its capabilities by answering world history questions in 10 languages at the
1958 World’s Fair. From 1956 to 1971 IBM sales had grown from $900 million to $8 billion, and its number of employees
had increased from 72,500 to 270,000. IBM had also innovated new marketing techniques such as the unbundling of
hardware, software, and computer services. So it was not a surprise that IBM would enter the fledgling but promising
personal computer business.

In fact, right from project conception, IBM took an intelligent approach to the personal computer field. It noticed that
the market for personal computers was spreading rapidly among both businesses and individuals. To move more rapidly than
usual, IBM recruited a team of 12 engineers to build a prototype computer. Once the project was approved, IBM picked
another small team of engineers to work on the project at its Boca Raton, Florida, laboratories. Philip Estridge,
manager of the project, owned an Apple II and appreciated its open architecture, which allowed for the easy development
of add-on products. IBM contracted with other companies to produce components for their computer and to base it on an
open architecture that could be built with commercially available materials. With this plan, IBM would be able to avoid
corporate bottlenecks and bring its computer to market in a year, more rapidly than competitors. Intel Corporation’s
16-bit 8088 microprocessor was selected as the central processing unit (CPU) for the computer, and for software IBM
turned to Microsoft Corporation. Until then the small software company had concentrated mostly on computer languages,
but Bill Gates and Paul Allen found it impossible to turn down this opportunity. They purchased a small operating system
from another company and turned it into PC-DOS (or MS-DOS, or sometimes just DOS, for disk operating system), which
quickly became the standard operating system for the IBM Personal Computer. IBM had first approached Digital Research to
inquire about its CP/M operating system, but Digital’s executives balked at signing IBM’s nondisclosure agreement. Later
IBM also offered a version of CP/M but priced it higher than DOS, sealing the fate of the operating system. In reality,
DOS resembled CP/M in both function and appearance, and users of CP/M found it easy to convert to the new IBM machines.

IBM had the benefit of its own experience to know that software was needed to make a computer useful. In preparation for
the release of its computer, IBM contracted with several software companies to develop important applications. From day
one it made available a word processor, a spreadsheet program, and a series of business programs. Personal computers
were just starting to gain acceptance in businesses, and in this market IBM had a built-in advantage, as expressed in
the adage “Nobody was ever fired for buying from IBM.”

IBM named its product the IBM Personal Computer, which quickly was shortened to the IBM PC. It was an immediate success,
selling more than 500,000 units in its first two years. More powerful than other desktop computers at the time, it came
with 16 kilobytes of memory (expandable to 256 kilobytes), one or two floppy disk drives, and an optional color monitor.
The giant company also took an unlikely but wise marketing approach by selling the IBM PC through computer dealers and
in department stores, something it had never done before.

IBM’s entry into personal computers broadened the market and energized the industry. Software developers, aware of Big
Blue’s immense resources and anticipating that the PC would be successful, set out to write programs for the computer.
Even competitors benefited from the attention that IBM brought to the field; and when they realized that they could
build machines compatible with the IBM PC, the industry rapidly changed.

The market expands
PC clones
Compaq portable computer
Compaq portable computerCompaq Computer Corporation introduced the first IBM-compatible portable computer in November
1982. At a weight of about 11 kg (25 pounds), it was sometimes referred to as a “luggable” computer.
In 1982 a well-funded start-up firm called Compaq Computer Corporation came out with a portable computer that was
compatible with the IBM PC. These first portables resembled sewing machines when they were closed and weighed about 13
kg (approximately 28 pounds)—at the time a true lightweight. Compatibility with the IBM PC meant that any software or
peripherals, such as printers, developed for use with the IBM PC would also work on the Compaq portable. The machine
caught IBM by surprise and was an immediate success. Compaq was not only successful but showed other firms how to
compete with IBM. Quickly thereafter many computer firms began offering “PC clones.” IBM’s decision to use off-the-shelf
parts, which once seemed brilliant, had altered the company’s ability to control the computer industry as it always had
with previous generations of technology.

The change also hurt Apple, which found itself isolated as the only company not sharing in the standard PC design.
Apple’s Macintosh was successful, but it could never hope to attract the customer base of all the companies building IBM
PC compatibles. Eventually software companies began to favor the PC makers with more of their development efforts, and
Apple’s market share began to drop. Apple cofounder Steve Wozniak left in February 1985 to become a teacher, and Apple
cofounder Steve Jobs was ousted in a power struggle in September 1985. During the ensuing turmoil, Apple held on to its
loyal customer base, thanks to its innovative user interface and overall ease of use, but its market share continued to
erode as lower-costing PCs began to catch up with, and even pass, Apple’s technological lead.

Microsoft’s Windows operating system
In 1985 Microsoft came out with its Windows operating system, which gave PC compatibles some of the same capabilities as
the Macintosh. Year after year, Microsoft refined and improved Windows so that Apple, which failed to come up with a
significant new advantage, lost its edge. IBM tried to establish yet another operating system, OS/2, but lost the battle
to Gates’s company. In fact, Microsoft also had established itself as the leading provider of application software for
the Macintosh. Thus Microsoft dominated not only the operating system and application software business for
PC-compatibles but also the application software business for the only nonstandard system with any sizable share of the
desktop computer market. In 1998, amid a growing chorus of complaints about Microsoft’s business tactics, the U.S.
Department of Justice filed a lawsuit charging Microsoft with using its monopoly position to stifle competition.

Workstation computers
While the personal computer market grew and matured, a variation on its theme grew out of university labs and began to
threaten the minicomputers for their market. The new machines were called workstations. They looked like personal
computers, and they sat on a single desktop and were used by a single individual just like personal computers, but they
were distinguished by being more powerful and expensive, by having more complex architectures that spread the
computational load over more than one CPU chip, by usually running the UNIX operating system, and by being targeted to
scientists and engineers, software and chip designers, graphic artists, moviemakers, and others needing high
performance. Workstations existed in a narrow niche between the cheapest minicomputers and the most powerful personal
computers, and each year they had to become more powerful, pushing at the minicomputers even as they were pushed at by
the high-end personal computers.

The most successful of the workstation manufacturers were Sun Microsystems, Inc., started by people involved in
enhancing the UNIX operating system, and, for a time, Silicon Graphics, Inc., which marketed machines for video and
audio editing.

The microcomputer market now included personal computers, software, peripheral devices, and workstations. Within two
decades this market had surpassed the market for mainframes and minicomputers in sales and every other measure. As if to
underscore such growth, in 1996 Silicon Graphics, a workstation manufacturer, bought the star of the supercomputer
manufacturers, Cray Research, and began to develop supercomputers as a sideline. Moreover, Compaq Computer
Corporation—which had parlayed its success with portable PCs into a perennial position during the 1990s as the leading
seller of microcomputers—bought the reigning king of the minicomputer manufacturers, Digital Equipment Corporation
(DEC). Compaq announced that it intended to fold DEC technology into its own expanding product line and that the DEC
brand name would be gradually phased out. Microcomputers were not only outselling mainframes and minis, they were
blotting them out.

Living in cyberspace
Ever smaller computers
Embedded systems
One can look at the development of the electronic computer as occurring in waves. The first large wave was the mainframe
era, when many people had to share single machines. (The mainframe era is covered in the section The age of Big Iron.)
In this view, the minicomputer era can be seen as a mere eddy in the larger wave, a development that allowed a favored
few to have greater contact with the big machines. Overall, the age of mainframes could be characterized by the
expression “Many persons, one computer.”

The second wave of computing history was brought on by the personal computer, which in turn was made possible by the
invention of the microprocessor. (This era is described in the section The personal computer revolution.) The impact of
personal computers has been far greater than that of mainframes and minicomputers: their processing power has overtaken
that of the minicomputers, and networks of personal computers working together to solve problems can be the equal of the
fastest supercomputers. The era of the personal computer can be described as the age of “One person, one computer.”

Since the introduction of the first personal computer, the semiconductor business has grown to be more than a $500
billion worldwide industry. The greatest growth in the semiconductor industry has occurred in the automotive, wireless,
and data storage sectors, according to a 2022 McKinsey & Company report. These computer chips are embedded in a vast
array of consumer devices, including smartphones, cars, televisions, kitchen appliances, video games, and toys.
Microchips can even be embedded into electric toothbrushes. Samsung, Intel and Taiwan Semiconductor Manufacturing
Company (TSMC) dominate the worldwide microprocessor industry. This ongoing third wave may be characterized as “One
person, many computers.”

Handheld digital devices
The origins of handheld digital devices go back to the 1960s, when Alan Kay, a researcher at Xerox’s Palo Alto Research
Center (PARC), promoted the vision of a small, powerful notebook-style computer that he called the Dynabook. Kay never
actually built a Dynabook (the technology had yet to be invented), but his vision helped to catalyze the research that
would eventually make his dream feasible.

It happened by small steps. The popularity of the personal computer and the ongoing miniaturization of the semiconductor
circuitry and other devices first led to the development of somewhat smaller, portable—or, as they were sometimes
called, luggable—computer systems. The first of these, the Osborne 1, designed by Lee Felsenstein, an electronics
engineer active in the Homebrew Computer Club in San Francisco, was sold in 1981. Soon most PC manufacturers had
portable models. At first these portables looked like sewing machines and weighed in excess of 9 kg (20 pounds).
Gradually they became smaller (laptop-, notebook-, and then sub-notebook-size) and came with more powerful processors.
These devices allowed people to use computers not only in the office or at home but also while traveling—on airplanes,
in waiting rooms, or even at the beach.

As the size of computers continued to shrink and microprocessors became more and more powerful, researchers and
entrepreneurs explored new possibilities in mobile computing. In the late 1980s and early ’90s, several companies came
out with handheld computers, called personal digital assistants (PDAs). PDAs typically replaced the cathode-ray-tube
screen with a more compact liquid crystal display, and they either had a miniature keyboard or replaced the keyboard
with a stylus and handwriting-recognition software that allowed the user to write directly on the screen. Like the first
personal computers, PDAs were built without a clear idea of what people would do with them. In fact, people did not do
much at all with the early models. To some extent, the early PDAs, made by Go Corporation and Apple, were
technologically premature; with their unreliable handwriting recognition, they offered little advantage over
paper-and-pencil planning books.

Palm Pilot
Palm Pilot Introduced in March 1997, the Palm Pilot personal digital assistant (PDA) was equipped with enough processing
power to store and manipulate personal information as well as handle the most common scheduling tasks.
The potential of this new kind of device was realized in 1996 when Palm Computing, Inc., released the Palm Pilot, which
was about the size of a deck of playing cards and sold for about $400—approximately the same price as the MITS Altair,
the first personal computer sold as a kit in 1974. The Pilot did not try to replace the computer but made it possible to
organize and carry information with an electronic calendar, telephone number and address list, memo pad, and
expense-tracking software and to synchronize that data with a PC. The device included an electronic cradle to connect to
a PC and pass information back and forth. It also featured a data-entry system called “graffiti,” which involved writing
with a stylus using a slightly altered alphabet that the device recognized. Its success encouraged numerous software
companies to develop applications for it.

BlackBerry
BlackBerryThe BlackBerry personal digital assistant (PDA), manufactured by the Canadian company Research in Motion.
In 1998 this market heated up further with the entry of several established consumer electronics firms using Microsoft’s
Windows CE operating system (a stripped-down version of the Windows system) to sell handheld computer devices and
wireless telephones that could connect to PCs. These small devices also often possessed a communications component and
benefited from the sudden popularization of the Internet and the World Wide Web. In particular, the BlackBerry PDA,
introduced by the Canadian company Research in Motion in 2002, established itself as a favorite in the corporate world
because of features that allowed employees to make secure connections with their company’s databases.

iPod
iPodApple's fifth-generation iPod portable media player, 2005.
In 2001 Apple introduced the iPod, a handheld device capable of storing 1,000 songs for playback. Apple quickly came to
dominate a booming market for music players. The iPod could also store notes and appointments. In 2003 Apple opened an
online music store, iTunes Store, and in the following software releases added photographs and movies to the media the
iPod could handle. The market for iPods and iPod-like devices was second only to cellular telephones among handheld
electronic devices.

LV enV2
LV enV2LG enV2 telephone by Verizon Wireless featuring a music player, games, mobile messaging, a camera, and a
camcorder.
While Apple and competitors grew the market for handheld devices with these media players, mobile telephones were
increasingly becoming “smartphones,” acquiring more of the functions of computers, including the ability to send and
receive email and text messages and to access the Internet. In 2007 Apple once again shook up a market for handheld
devices, this time redefining the smartphone market with its iPhone. The touch-screen interface of the iPhone was in its
way more advanced than the graphical user interface used on personal computers, its storage rivaled that of computers
from just a few years before, and its operating system was a modified version of the operating system on the Apple
Macintosh. This, along with synchronizing and distribution technology, embodied a vision of ubiquitous computing in
which personal documents and other media could be moved easily from one device to another. Handheld devices and
computers found their link through the Internet.

One interconnected world
The Internet
The Internet grew out of funding by the U.S. Advanced Research Projects Agency (ARPA), later renamed the Defense
Advanced Research Projects Agency (DARPA), to develop a communication system among government and academic
computer-research laboratories. The first network component, ARPANET, became operational in October 1969. With only 15
nongovernment (university) sites included in ARPANET, the U.S. National Science Foundation decided to fund the
construction and initial maintenance cost of a supplementary network, the Computer Science Network (CSNET). Built in
1980, CSNET was made available, on a subscription basis, to a wide array of academic, government, and industry research
labs. As the 1980s wore on, further networks were added. In North America there were (among others): BITNET (Because
It’s Time Network) from IBM, UUCP (UNIX-to-UNIX Copy Protocol) from Bell Telephone, USENET (initially a connection
between Duke University, Durham, North Carolina, and the University of North Carolina and still the home system for the
Internet’s many newsgroups), NSFNET (a high-speed National Science Foundation network connecting supercomputers), and
CDNet (in Canada). In Europe several small academic networks were linked to the growing North American network.

All these various networks were able to communicate with one another because of two shared protocols: the
Transmission-Control Protocol (TCP), which split large files into numerous small files, or packets, assigned sequencing
and address information to each packet, and reassembled the packets into the original file after arrival at their final
destination; and the Internet Protocol (IP), a hierarchical addressing system that controlled the routing of packets
(which might take widely divergent paths before being reassembled).

What it took to turn a network of computers into something more was the idea of the hyperlink: computer code inside a
document that would cause related documents to be fetched and displayed. The concept of hyperlinking was anticipated
from the early to the middle decades of the 20th century—in Belgium by Paul Otlet and in the United States by Ted
Nelson, Vannevar Bush, and, to some extent, Douglas Engelbart. Their yearning for some kind of system to link knowledge
together, though, did not materialize until 1990, when Tim Berners-Lee of England and others at CERN (European
Organization for Nuclear Research) developed a protocol based on hypertext to make information distribution easier. In
1991 this culminated in the creation of the World Wide Web and its system of links among user-created pages. A team of
programmers at the U.S. National Center for Supercomputing Applications, Urbana, Illinois, developed a program called a
browser that made it easier to use the World Wide Web, and a spin-off company named Netscape Communications Corp. was
founded to commercialize that technology.

Netscape was an enormous success. The Web grew exponentially, doubling the number of users and the number of sites every
few months. Uniform resource locators (URLs) became part of daily life, and the use of electronic mail (email) became
commonplace. Increasingly business took advantage of the Internet and adopted new forms of buying and selling in
“cyberspace.” (Science fiction author William Gibson popularized this term in the early 1980s.) With Netscape so
successful, Microsoft and other firms developed alternative Web browsers.

Originally created as a closed network for researchers, the Internet was suddenly a new public medium for information.
It became the home of virtual shopping malls, bookstores, stockbrokers, newspapers, and entertainment. Schools were
“getting connected” to the Internet, and children were learning to do research in novel ways. The combination of the
Internet, email, and small and affordable computing and communication devices began to change many aspects of society.

It soon became apparent that new software was necessary to take advantage of the opportunities created by the Internet.
Sun Microsystems, maker of powerful desktop computers known as workstations, invented a new object-oriented programming
language called Java. Meeting the design needs of embedded and networked devices, this new language was aimed at making
it possible to build applications that could be stored on one system but run on another after passing over a network.
Alternatively, various parts of applications could be stored in different locations and moved to run in a single device.
Java was one of the more effective ways to develop software for “smart cards,” plastic debit cards with embedded
computer chips that could store and transfer electronic funds in place of cash.

E-commerce
Early enthusiasm over the potential profits from e-commerce led to massive cash investments and a “dot-com”
boom-and-bust cycle in the 1990s. By the end of the decade, half of these businesses had failed, though certain
successful categories of online business had been demonstrated, and most conventional businesses had established an
online presence. Search and online advertising proved to be the most successful new business areas.

Some online businesses created niches that did not exist before. eBay, founded in 1995 as an online auction and shopping
website, gave members the ability to set up their own stores online. Although sometimes criticized for not creating any
new wealth or products, eBay made it possible for members to run small businesses from their homes without a large
initial investment. In 2003 Linden Research, Inc., launched Second Life, an Internet-based virtual reality world in
which participants (called “residents”) have cartoonlike avatars that move through a graphical environment. Residents
socialize, participate in group activities, and create and trade virtual products and virtual or real services. Second
Life has its own currency, the Linden Dollar, which can be converted to U.S. dollars at several Internet currency
exchange markets.

Maintaining an Internet presence became common for conventional businesses during the 1990s and 2000s as they sought to
reach out to a public that was increasingly active in online social communities. In addition to seeking some way of
responding to the growing numbers of their customers who were sharing their experiences with company products and
services online, companies discovered that many potential customers searched online for the best deals and the locations
of nearby businesses. With an Internet-enabled smartphone, a customer might, for example, check for nearby restaurants
using its built-in access to the Global Positioning System (GPS), check a map on the Web for directions to the
restaurant, and then call for a reservation, all while en route.

The growth of online business was accompanied, though, by a rise in cybercrime, particularly identity theft, in which a
criminal might gain access to someone’s credit card or other identification and use it to make purchases.

Social networking
Social networking services emerged as a significant online phenomenon in the 2000s. These services used software to
facilitate online communities, where members with shared interests swapped files, photographs, videos, and music, sent
messages and chatted, set up blogs (Web diaries) and discussion groups, and shared opinions. Early social networking
services included Classmates.com, which connected former schoolmates, and Yahoo! 360°, Myspace, and SixDegrees, which
built networks of connections via friends of friends. By 2018 the leading social networking services included Facebook,
Twitter, Instagram, LinkedIn, and Snapchat. LinkedIn became an effective tool for business staff recruiting. Businesses
began exploring how to exploit these networks, drawing on social networking research and theory which suggested that
finding key “influential” members of existing networks of individuals could give access to and credibility with the
whole network.

Blogs became a category unto themselves, and some blogs had thousands of participants. Trust became a commodity, as
sharing opinions or ratings proved to be a key to effective blog discussions, as well as an important component of many
e-commerce websites. Daily Kos, one of the largest of the political blogs, made good use of ratings, with high-rated
members gaining more power to rate other members’ comments; under such systems, the idea is that the best entries will
survive and the worst will quickly disappear. The vendor rating system in eBay similarly allowed for a kind of
self-policing that was intended to weed out unethical or otherwise undesirable vendors.

Ubiquitous computing
The combination of the connectedness of the Internet with the ability of new microprocessors that can handle multiple
tasks in parallel has inspired new ways of programming. Programmers are developing software to divide computational
tasks into subtasks that a program can assign to separate processors in order to achieve greater efficiency and speed.
This trend is one of various ways that computers are being connected to share information and to solve complex problems.
In such distributed computing applications as airline reservation systems and automated teller machines, data pass
through networks connected all over the world. Distributed computing promises to make better use of computers connected
to ever larger and more complex networks. In effect, a distributed network of personal computers becomes a
supercomputer. Many applications, such as research into protein folding, have been done on distributed networks, and
some of these applications have involved calculations that would be too demanding for any single computer in existence.

Considerable work in research laboratories is extending the actual development of embedded microprocessors to a more
sweeping vision in which these chips will be found everywhere and will meet human needs wherever people go. For
instance, the Global Positioning System (GPS)—a satellite communication and positioning system developed for the U.S.
military—is now accessible by anyone, anywhere in the world, via a smartphone. In conjunction with various
computer-mapping softwares, GPS can be used to locate one’s position and plan a travel route, whether by car, by public
transit, or on foot.

Some researchers call this trend ubiquitous computing or pervasive computing. Ubiquitous computing would extend the
increasingly networked world and the powerful capabilities of distributed computing—i.e., the sharing of computations
among microprocessors connected over a network. (The use of multiple microprocessors within one machine is discussed in
the article supercomputer.) With more powerful computers, all connected all the time, thinking machines would be
involved in every facet of human life, albeit invisibly.

Xerox PARC’s vision and research in the 1960s and ’70s eventually achieved commercial success in the form of the
mouse-driven graphical user interface, networked computers, laser printers, and notebook-style machines. Today the
vision of ubiquitous computing foresees a day when microprocessors will be found wherever humans go. The technology will
be invisible and natural and will respond to normal patterns of behavior. Computers will disappear, or rather become a
transparent part of the physical environment, thus truly bringing about an era of “One person, many computers.”

Paul A. Freiberger
Michael R. Swaine
artificial intelligence
Table of Contents
Introduction & Top Questions
What is intelligence?
Methods and goals in AI
AI technology
Is artificial general intelligence (AGI) possible?
Want to learn more?
References & Edit History
Quick Facts & Related Topics
Images & Videos
artificial intelligenceGarry Kasparov and Deep BlueTikTok account featuring a deepfake of Keanu ReevesConversation with
ChatGPTGoogle data centerAttila the robotGenghis the robotHerbert the robotthree stages of mobile robot development for
the Mars Rover Research ProjectPebbles the robot
For Students
Pebbles the robot
artificial intelligence summary
Quizzes
computer chip. computer. Hand holding computer chip. Central processing unit (CPU). history and society, science and
technology, microchip, microprocessor motherboard computer Circuit Board
Computers and Technology Quiz
Related Questions
What is artificial intelligence?
Read Next
Grade school students working at computers in a school library. Study learn girl child class technology
The Future of Information and Education
Motion Abstract background binary code ,futuristic Design Abstract Wave line Infinite Loop for Business science and
Technology, articiial intelligence, algorithm
Pro and Con: Artificial Intelligence
Abstract vector hi speed internet technology background
History of Technology Timeline
Discover
Jackson (Wyoming, United States). Jackson Lake (also called Jackson Hole), southern end of the Teton Range (the Grand
Tetons), Grand Teton National Park, Wyoming, USA
7 Wonders of America
Sailboat against a beautiful landscape
Which Waters Do You Pass Through When You “Sail the Seven Seas”?
Small, white rat (genus Rattus) on a glass table. (rodent, laboratory, experiment)
Cruel and Unusual Punishments: 15 Types of Torture
Adolf Hitler (Nazi, nazism, German leader).
9 Things You Might Not Know About Adolf Hitler
Shah Jahan. Taj Mahal. Mughal architecture. Emperor Shah Jahan fifth Mughal Emperor (reigned 1628-1658) India, Himachal
Pradesh, Basohli or Jammu and Kashmir, Mankot, circa 1690 Drawings; Opaque watercolor, gold, and ink on paper (see
notes)
6 Important Mughal Emperors
Calendar showing the month of February
Why Are There Only 28 Days in February?
pg 229Nazi parade features a banner proclaiming, "Death to Marxism."The possibility of a peaceful Germany after World
War I was precluded entirely by the terms of the Versailles Treaty and theintransigent hostility of France and England.
Stripped of indu
Were the Nazis Socialists?
Technology
Computers
artificial intelligence
artificial intelligence Image generated by the Stable Diffusion model from the prompt “the ability of a digital computer
or computer-controlled robot to perform tasks commonly associated with intelligent beings,” which is the definition of
artificial intelligence (AI) in the Encyclopædia Britannica article on the subject. Stable Diffusion is trained on a
large set of images paired with textual descriptions and uses natural language processing (NLP) to generate an image.
artificial intelligence
Also known as: AI
Written by
Fact-checked by
Last Updated: Feb 24, 2025 • Article History
Key People: Geoffrey Hinton John M. Jumper Marvin Minsky Edward Albert Feigenbaum Allen Newell
Related Topics: history of artificial intelligence (AI) computational aesthetics prompt engineering three laws of
robotics generative AI
Top Questions
What is artificial intelligence?
Are artificial intelligence and machine learning the same?
News • Chip Ganassi Racing partners with OpenAI in first motorsports venture for AI company • Feb. 28, 2025, 5:14 PM ET
(AP)
artificial intelligence (AI), the ability of a digital computer or computer-controlled robot to perform tasks commonly
associated with intelligent beings. The term is frequently applied to the project of developing systems endowed with the
intellectual processes characteristic of humans, such as the ability to reason, discover meaning, generalize, or learn
from past experience. Since their development in the 1940s, digital computers have been programmed to carry out very
complex tasks—such as discovering proofs for mathematical theorems or playing chess—with great proficiency. Despite
continuing advances in computer processing speed and memory capacity, there are as yet no programs that can match full
human flexibility over wider domains or in tasks requiring much everyday knowledge. On the other hand, some programs
have attained the performance levels of human experts and professionals in executing certain specific tasks, so that
artificial intelligence in this limited sense is found in applications as diverse as medical diagnosis, computer search
engines, voice or handwriting recognition, and chatbots.

What is intelligence?
What do you think?
Is Artificial Intelligence Good for Society?
Explore the ProCon debate

All but the simplest human behavior is ascribed to intelligence, while even the most complicated insect behavior is
usually not taken as an indication of intelligence. What is the difference? Consider the behavior of the digger wasp,
Sphex ichneumoneus. When the female wasp returns to her burrow with food, she first deposits it on the threshold, checks
for intruders inside her burrow, and only then, if the coast is clear, carries her food inside. The real nature of the
wasp’s instinctual behavior is revealed if the food is moved a few inches away from the entrance to her burrow while she
is inside: on emerging, she will repeat the whole procedure as often as the food is displaced.
Intelligence—conspicuously absent in the case of the wasp—must include the ability to adapt to new circumstances.

Psychologists generally characterize human intelligence not by just one trait but by the combination of many diverse
abilities. Research in AI has focused chiefly on the following components of intelligence: learning, reasoning, problem
solving, perception, and using language.

Learning
There are a number of different forms of learning as applied to artificial intelligence. The simplest is learning by
trial and error. For example, a simple computer program for solving mate-in-one chess problems might try moves at random
until mate is found. The program might then store the solution with the position so that, the next time the computer
encountered the same position, it would recall the solution. This simple memorizing of individual items and
procedures—known as rote learning—is relatively easy to implement on a computer. More challenging is the problem of
implementing what is called generalization. Generalization involves applying past experience to analogous new
situations. For example, a program that learns the past tense of regular English verbs by rote will not be able to
produce the past tense of a word such as jump unless the program was previously presented with jumped, whereas a program
that is able to generalize can learn the “add -ed” rule for regular verbs ending in a consonant and so form the past
tense of jump on the basis of experience with similar verbs.

(Read Ray Kurzweil’s Britannica essay on the future of “Nonbiological Man.”)

computer chip. computer. Hand holding computer chip. Central processing unit (CPU). history and society, science and
technology, microchip, microprocessor motherboard computer Circuit Board
Britannica Quiz
Computers and Technology Quiz
Reasoning
AI & your money
Artificial intelligence is changing how we interact online, how we manage our finances, and even how we work. Learn more
with Britannica Money.

Using AI for money management
How AI is changing work
Ethical questions and AI
AI and regulation
Investing in AI stocks
To reason is to draw inferences appropriate to the situation. Inferences are classified as either deductive or
inductive. An example of the former is, “Fred must be in either the museum or the café. He is not in the café;
therefore, he is in the museum,” and of the latter is, “Previous accidents of this sort were caused by instrument
failure. This accident is of the same sort; therefore, it was likely caused by instrument failure.” The most significant
difference between these forms of reasoning is that in the deductive case, the truth of the premises guarantees the
truth of the conclusion, whereas in the inductive case, the truth of the premises lends support to the conclusion
without giving absolute assurance. Inductive reasoning is common in science, where data are collected and tentative
models are developed to describe and predict future behavior—until the appearance of anomalous data forces the model to
be revised. Deductive reasoning is common in mathematics and logic, where elaborate structures of irrefutable theorems
are built up from a small set of basic axioms and rules.

There has been considerable success in programming computers to draw inferences. However, true reasoning involves more
than just drawing inferences: it involves drawing inferences relevant to the solution of the particular problem. This is
one of the hardest problems confronting AI.

Problem solving
Problem solving, particularly in artificial intelligence, may be characterized as a systematic search through a range of
possible actions in order to reach some predefined goal or solution. Problem-solving methods divide into special purpose
and general purpose. A special-purpose method is tailor-made for a particular problem and often exploits very specific
features of the situation in which the problem is embedded. In contrast, a general-purpose method is applicable to a
wide variety of problems. One general-purpose technique used in AI is means-end analysis—a step-by-step, or incremental,
reduction of the difference between the current state and the final goal. The program selects actions from a list of
means—in the case of a simple robot, this might consist of PICKUP, PUTDOWN, MOVEFORWARD, MOVEBACK, MOVELEFT, and
MOVERIGHT—until the goal is reached.

Many diverse problems have been solved by artificial intelligence programs. Some examples are finding the winning move
(or sequence of moves) in a board game, devising mathematical proofs, and manipulating “virtual objects” in a
computer-generated world.

Perception
In perception the environment is scanned by means of various sensory organs, real or artificial, and the scene is
decomposed into separate objects in various spatial relationships. Analysis is complicated by the fact that an object
may appear different depending on the angle from which it is viewed, the direction and intensity of illumination in the
scene, and how much the object contrasts with the surrounding field. At present, artificial perception is sufficiently
advanced to enable optical sensors to identify individuals and enable autonomous vehicles to drive at moderate speeds on
the open road.

Language
A language is a system of signs having meaning by convention. In this sense, language need not be confined to the spoken
word. Traffic signs, for example, form a mini-language, it being a matter of convention that ⚠ means “hazard ahead” in
some countries. It is distinctive of languages that linguistic units possess meaning by convention, and linguistic
meaning is very different from what is called natural meaning, exemplified in statements such as “Those clouds mean
rain” and “The fall in pressure means the valve is malfunctioning.”

An important characteristic of full-fledged human languages—in contrast to birdcalls and traffic signs—is their
productivity. A productive language can formulate an unlimited variety of sentences.

Large language models like ChatGPT can respond fluently in a human language to questions and statements. Although such
models do not actually understand language as humans do but merely select words that are more probable than others, they
have reached the point where their command of a language is indistinguishable from that of a normal human. What, then,
is involved in genuine understanding, if even a computer that uses language like a native human speaker is not
acknowledged to understand? There is no universally agreed upon answer to this difficult question.

(Read Yuval Noah Harari’s Britannica essay on the future of “Nonconscious Man.”)

Methods and goals in AI
Symbolic vs. connectionist approaches
AI research follows two distinct, and to some extent competing, methods, the symbolic (or “top-down”) approach, and the
connectionist (or “bottom-up”) approach. The top-down approach seeks to replicate intelligence by analyzing cognition
independent of the biological structure of the brain, in terms of the processing of symbols—whence the symbolic label.
The bottom-up approach, on the other hand, involves creating artificial neural networks in imitation of the brain’s
structure—whence the connectionist label.

To illustrate the difference between these approaches, consider the task of building a system, equipped with an optical
scanner, that recognizes the letters of the alphabet. A bottom-up approach typically involves training an artificial
neural network by presenting letters to it one by one, gradually improving performance by “tuning” the network. (Tuning
adjusts the responsiveness of different neural pathways to different stimuli.) In contrast, a top-down approach
typically involves writing a computer program that compares each letter with geometric descriptions. Simply put, neural
activities are the basis of the bottom-up approach, while symbolic descriptions are the basis of the top-down approach.

In The Fundamentals of Learning (1932), Edward Thorndike, a psychologist at Columbia University, New York City, first
suggested that human learning consists of some unknown property of connections between neurons in the brain. In The
Organization of Behavior (1949), Donald Hebb, a psychologist at McGill University, Montreal, suggested that learning
specifically involves strengthening certain patterns of neural activity by increasing the probability (weight) of
induced neuron firing between the associated connections.

In 1957 two vigorous advocates of symbolic AI—Allen Newell, a researcher at the RAND Corporation, Santa Monica,
California, and Herbert Simon, a psychologist and computer scientist at Carnegie Mellon University, Pittsburgh—summed up
the top-down approach in what they called the physical symbol system hypothesis. This hypothesis states that processing
structures of symbols is sufficient, in principle, to produce artificial intelligence in a digital computer and that,
moreover, human intelligence is the result of the same type of symbolic manipulations.

During the 1950s and ’60s the top-down and bottom-up approaches were pursued simultaneously, and both achieved
noteworthy, if limited, results. During the 1970s, however, bottom-up AI was neglected, and it was not until the 1980s
that this approach again became prominent. Nowadays both approaches are followed, and both are acknowledged as facing
difficulties. Symbolic techniques work in simplified realms but typically break down when confronted with the real
world; meanwhile, bottom-up researchers have been unable to replicate the nervous systems of even the simplest living
things. Caenorhabditis elegans, a much-studied worm, has approximately 300 neurons whose pattern of interconnections is
perfectly known. Yet connectionist models have failed to mimic even this worm. Evidently, the neurons of connectionist
theory are gross oversimplifications of the real thing.

computer chip. computer. Hand holding computer chip. Central processing unit (CPU). history and society, science and
technology, microchip, microprocessor motherboard computer Circuit Board
Britannica Quiz
Computers and Technology Quiz
Artificial general intelligence (AGI), applied AI, and cognitive simulation
Employing the methods outlined above, AI research attempts to reach one of three goals: artificial general intelligence
(AGI), applied AI, or cognitive simulation. AGI (also called strong AI) aims to build machines that think. The ultimate
ambition of AGI is to produce a machine whose overall intellectual ability is indistinguishable from that of a human
being’s. To date, progress has been uneven. Despite advances in large-language models, it is debatable whether AGI can
emerge from even more powerful models or if a completely different approach is needed. Indeed, some researchers working
in AI’s other two branches view AGI as not worth pursuing.

Applied AI, also known as advanced information processing, aims to produce commercially viable “smart” systems—for
example, “expert” medical diagnosis systems and stock-trading systems. Applied AI has enjoyed considerable success.

In cognitive simulation, computers are used to test theories about how the human mind works—for example, theories about
how people recognize faces or recall memories. Cognitive simulation is already a powerful tool in both neuroscience and
cognitive psychology.

AI technology
In the early 21st century faster processing power and larger datasets (“big data”) brought artificial intelligence out
of computer science departments and into the wider world. Moore’s law, the observation that computing power doubled
roughly every 18 months, continued to hold true. The stock responses of the early chatbot Eliza fit comfortably within
50 kilobytes; the language model at the heart of ChatGPT was trained on 45 terabytes of text.

Machine learning
The ability of neural networks to take on added layers and thus work on more-complex problems increased in 2006 with the
invention of the “greedy layer-wise pretraining” technique, in which it was found that it was easier to train each layer
of a neural network individually than to train the whole network from input to output. This improvement in neural
network training led to a type of machine learning called “deep learning,” in which neural networks have four or more
layers, including the initial input and the final output. Moreover, such networks are able to learn unsupervised—that
is, to discover features in data without initial prompting.

Among the achievements of deep learning have been advances in image classification in which specialized neural networks
called convolution neural networks (CNNs) are trained on features found in a set of images of many different types of
objects. The CNN is then able to take an input image, compare it with features in images in its training set, and
classify the image as being of, for example, a cat or an apple. One such network, PReLU-net by Kaiming He and
collaborators at Microsoft Research, has classified images even better than a human did.

Garry Kasparov and Deep Blue
Garry Kasparov and Deep BlueWorld chess champion Garry Kasparov playing against Deep Blue, the chess-playing computer
built by IBM. In 1996 Kasparov won the first match 4−2, but in 1997 he lost to Deep Blue 3 ½−2 ½.
The achievement of Deep Blue in beating world chess champion Garry Kasparov was surpassed by DeepMind’s AlphaGo, which
mastered go, a much more complicated game than chess. AlphaGo’s neural networks learned to play go from human players
and by playing itself. It defeated top go player Lee Sedol 4–1 in 2016. AlphaGo was in turn surpassed by AlphaGo Zero,
which, starting from only the rules of go, was eventually able to defeat AlphaGo 100–0. A more general neural network,
Alpha Zero, was able to use the same techniques to quickly master chess and shogi.

Machine learning has found applications in many fields beyond gaming and image classification. The pharmaceutical
company Pfizer used the technique to quickly search millions of possible compounds in developing the COVID-19 treatment
Paxlovid. Google uses machine learning to filter out spam from the inbox of Gmail users. Banks and credit card companies
use historical data to train models to detect fraudulent transactions.

TikTok account featuring a deepfake of Keanu Reeves
TikTok account featuring a deepfake of Keanu ReevesThe “Unreal Keanu Reeves” TikTok account posts include relationship
humor and dances.
Deepfakes are AI-generated media produced using two different deep-learning algorithms: one that creates the best
possible replica of a real image or video and another that detects whether the replica is fake and, if it is, reports on
the differences between it and the original. The first algorithm produces a synthetic image and receives feedback on it
from the second algorithm; it then adjusts it to make it appear more real. The process is repeated until the second
algorithm does not detect any false imagery. Deepfake media portray images that do not exist in reality or events that
have never occurred. Widely circulated deepfakes include an image of Pope Francis in a puffer jacket, an image of former
U.S. president Donald Trump in a scuffle with police officers, and a video of Facebook CEO Mark Zuckerberg giving a
speech about his company’s nefarious power. Such events did not occur in real life.

Large language models and natural language processing
Natural language processing (NLP) involves analyzing how computers can process and parse language similarly to the way
humans do. To do this, NLP models must use computational linguistics, statistics, machine learning, and deep-learning
models. Early NLP models were hand-coded and rule-based but did not account for exceptions and nuances in language.
Statistical NLP was the next step, using probability to assign the likelihood of certain meanings to different parts of
text. Modern NLP systems use deep-learning models and techniques that help them to “learn” as they process information.

Conversation with ChatGPT
Conversation with ChatGPTScreenshot of a ChatGPT conversation created by Encyclopædia Britannica editor Erik Gregersen.
The conversation prompt was to write a 250-word encyclopaedia article about ChatGPT.
Prominent examples of modern NLP are language models that use AI and statistics to predict the final form of a sentence
on the basis of existing portions. In large language model (LLM), the word large refers to the parameters, or variables
and weights, used by the model to influence the prediction outcome. Although there is no definition for how many
parameters are needed, LLM training datasets range in size from 110 million parameters (Google’s BERTbase model) to 340
billion parameters (Google’s PaLM 2 model). Large also refers to the sheer amount of data used to train an LLM, which
can be multiple petabytes in size and contain trillions of tokens, which are the basic units of text or code, usually a
few characters long, that are processed by the model.

One popular language model was GPT-3, released by OpenAI in June 2020. One of the first LLMs, GPT-3 could solve
high-school-level math problems as well as create computer programs. GPT-3 was the foundation of ChatGPT software,
released in November 2022. ChatGPT almost immediately disturbed academics, journalists, and others because of concern
that it was impossible to distinguish human writing from ChatGPT-generated writing.

A flurry of LLMs and chatbots based on them followed in ChatGPT’s wake. Microsoft added the chatbot Copilot in 2023 to
its Windows 11 operating system, its Bing search engine, and its Edge browser. That same year, Google released a
chatbot, Bard (later Gemini), and in 2024, the company announced that “AI Overviews” of subjects would appear at the top
of search results.

One issue with LLMs is “hallucinations”: rather than communicating to a user that it does not know something, the model
responds with probable but inaccurate text based on the user’s prompts. This issue may be partially attributed to using
LLMs as search engines rather than in their intended role as text generators. One method to combat hallucinations is
known as prompt engineering, whereby engineers design prompts that aim to extract the optimal output from the model. For
example, one such prompt style is chain-of-thought, in which the initial prompt contains both an example question and a
carefully worked out answer to show the LLM how to proceed.

Other examples of machines using NLP are voice-operated GPS systems, customer service chatbots, and language translation
programs. In addition, businesses use NLP to enhance understanding of and service to consumers by auto-completing search
queries and monitoring social media.

Programs such as OpenAI’s DALL-E, Stable Diffusion, and Midjourney use NLP to create images based on textual prompts,
which can be as simple as “a red block on top of a green block” or as complex as “a cube with the texture of a
porcupine.” The programs are trained on large datasets with millions or billions of text-image pairs—that is, images
with textual descriptions.

NLP presents certain issues, especially as machine-learning algorithms and the like often express biases implicit in the
content on which they are trained. For example, when asked to describe a doctor, language models may be more likely to
respond with “He is a doctor” than “She is a doctor,” demonstrating inherent gender bias. Bias in NLP can have
real-world consequences. For instance, in 2015 Amazon’s NLP program for résumé screening to aid in the selection of job
candidates was found to discriminate against women, as women were underrepresented in the original training set
collected from employees.

Autonomous vehicles
Machine learning and AI are foundational elements of autonomous vehicle systems. Vehicles are trained on complex data
(e.g., the movement of other vehicles, road signs) with machine learning, which helps to improve the algorithms they
operate under. AI enables vehicles’ systems to make decisions without needing specific instructions for each potential
situation.

In order to make autonomous vehicles safe and effective, artificial simulations are created to test their capabilities.
To create such simulations, black-box testing is used, in contrast to white-box validation. White-box testing, in which
the internal structure of the system being tested is known to the tester, can prove the absence of failure. Black-box
methods are much more complicated and involve taking a more adversarial approach. In such methods, the internal design
of the system is unknown to the tester, who instead targets the external design and structure. These methods attempt to
find weaknesses in the system to ensure that it meets high safety standards.

As of 2024, fully autonomous vehicles are not available for consumer purchase. Certain obstacles have proved challenging
to overcome. For example, maps of almost four million miles of public roads in the United States would be needed for an
autonomous vehicle to operate effectively, which presents a daunting task for manufacturers. Additionally, the most
popular cars with a “self-driving” feature, those of Tesla, have raised safety concerns, as such vehicles have even
headed toward oncoming traffic and metal posts. AI has not progressed to the point where cars can engage in complex
interactions with other drivers or with cyclists or pedestrians. Such “common sense” is necessary to prevent accidents
and create a safe environment.

In October 2015 Google’s self-driving car, Waymo (which the company had been working on since 2009) completed its first
fully driverless trip with one passenger. The technology had been tested on one billion miles within simulations, and
two million miles on real roads. Waymo, which boasts a fleet of fully electric-powered vehicles, operates in San
Francisco and Phoenix, where users can call for a ride, much as with Uber or Lyft. The steering wheel, gas pedal, and
brake pedal operate without human guidance, differentiating the technology from Tesla’s autonomous driving feature.
Though the technology’s valuation peaked at $175 billion in November 2019, it had sunk to just $30 billion by 2020.
Waymo is being investigated by the U.S. National Highway Traffic Safety Administration (NHTSA) after more than 20
different reports of traffic violations. In certain cases, the vehicles drove on the wrong side of the road and in one
instance, hit a cyclist.

Virtual assistants
Virtual assistants (VAs) serve a variety of functions, including helping users schedule tasks, making and receiving
calls, and guiding users on the road. These devices require large amounts of data and learn from user input to become
more effective at predicting user needs and behavior. The most popular VAs on the market are Amazon Alexa, Google
Assistant, and Apple’s Siri. Virtual assistants differ from chatbots and conversational agents in that they are more
personalized, adapting to an individual user’s behavior and learning from it to improve over time.

Human-machine communication began in the 1960s with Eliza. PARRY, designed by the psychiatrist Kenneth Colby, followed
in the early 1970s and was designed to mimic a conversation with a person with paranoid schizophrenia. Simon, designed
by IBM in 1994, was one of the first devices that could technically be called a “smartphone,” and was marketed as a
personal digital assistant (PDA). Simon was the first device to feature a touchscreen, and it had email and fax
capability as well. Although Simon was not technically a VA, its development was essential in creating future
assistants. In February 2010 Siri, the first modern VA, was introduced for iOS, Apple’s mobile operating system, with
the iPhone 4S. Siri was the first VA able to be downloaded to a smartphone.

Voice assistants parse human speech by breaking it down into distinct sounds known as phonemes, using an automatic
speech recognition (ASR) system. After breaking down the speech, the VA analyzes and “remembers” the tone and other
aspects of the voice to recognize the user. Over time, VAs have become more sophisticated through machine learning, as
they have access to many millions of words and phrases. In addition, they often use the Internet to find answers to user
questions—for example, when a user asks for a weather forecast.

Risks
AI poses certain risks in terms of ethical and socioeconomic consequences. As more tasks become automated, especially in
such industries as marketing and health care, many workers are poised to lose their jobs. Although AI may create some
new jobs, these may require more technical skills than the jobs AI has replaced.

Moreover, AI has certain biases that are difficult to overcome without proper training. For example, U.S. police
departments have begun using predictive policing algorithms to indicate where crimes are most likely to occur. However,
such systems are based partly on arrest rates, which are already disproportionately high in Black communities. This may
lead to over-policing in such areas, which further affects these algorithms. As humans are inherently biased, algorithms
are bound to reflect human biases.

Privacy is another aspect of AI that concerns experts. As AI often involves collecting and processing large amounts of
data, there is the risk that this data will be accessed by the wrong people or organizations. With generative AI, it is
even possible to manipulate images and create fake profiles. AI can also be used to survey populations and track
individuals in public spaces. Experts have implored policymakers to develop practices and policies that maximize the
benefits of AI while minimizing the potential risks. In January 2024 singer Taylor Swift was the target of sexually
explicit non-consensual deepfakes that were widely circulated on social media. Many individuals had already faced this
type of online abuse (made possible by AI), but Swift’s status brought the issue to the forefront of public policy.

Google data center
Google data centerAerial view of a Google data center complex in Eemshaven, Netherlands.
LLMs are located at data centers that require large amounts of electricity. In 2020 Microsoft pledged that it would be
carbon neutral by 2030. In 2024 it announced that in the previous fiscal year its carbon emissions had increased by
almost 30 percent, mostly from the building materials and hardware required in building more data centers. A ChatGPT
query requires about 10 times more electricity than a Google Search. Goldman Sachs has estimated that data centers will
use about 8 percent of U.S. electricity in 2030.

As of 2024 there are few laws regulating AI. Existing laws such as the European Union’s General Data Protection
Regulation (GDPR) and the California Consumer Privacy Act (CCPA) do govern AI models but only insofar as they use
personal information. The most wide-reaching regulation is the EU’s AI Act, which passed in March 2024. Under the AI
Act, models that perform social scoring of citizens’ behavior and characteristics and that attempt to manipulate users’
behavior are banned. AI models that deal with “high-risk” subjects, such as law enforcement and infrastructure, must be
registered in an EU database.

AI has also led to issues concerning copyright law and policy. In 2023 the U.S. government Copyright Office began an
initiative to investigate the issue of AI using copyrighted works to generate content. That year almost 15 new cases of
copyright-related suits were filed against companies involved in creating generative AI programs. One prominent company,
Stability AI, came under fire for using unlicensed images to generate new content. Getty Images, which filed the suit,
added its own AI feature to its platform, partially in response to the host of services that offer “stolen imagery.”
There are also questions of whether work created by AI is worthy of a copyright label. Currently, AI-made content cannot
be copyrighted, but there are arguments for and against copyrighting it.

Although many AI companies claim that their content does not require human labor, in many cases, such “groundbreaking”
technology is reliant on exploited workers from developing countries. For example, a Time magazine investigation found
that OpenAI had used Kenyan workers (who had been paid less than $2 an hour) to sort through text snippets in order to
help remove toxic and sexually explicit language from ChatGPT. The project was canceled in February 2022 because of how
traumatic the task was for workers. Although Amazon had marketed its Amazon Go cashier-less stores as being fully
automated (e.g., its AI could detect the items in a customer’s basket), it was revealed that the “Just Walk Out”
technology was actually powered by outsourced labor from India, where more than a thousand workers operated as “remote
cashiers,” leading to the joke that, in this case, AI stood for Actually Indians.

Is artificial general intelligence (AGI) possible?
What do you think?
Is Artificial Intelligence Good for Society?
Explore the ProCon debate

Artificial general intelligence (AGI), or strong AI—that is, artificial intelligence that aims to duplicate human
intellectual abilities—remains controversial and out of reach. The difficulty of scaling up AI’s modest achievements
cannot be overstated.

However, this lack of progress may simply be testimony to the difficulty of AGI, not to its impossibility. Let us turn
to the very idea of AGI. Can a computer possibly think? The theoretical linguist Noam Chomsky suggests that debating
this question is pointless, for it is an essentially arbitrary decision whether to extend common usage of the word think
to include machines. There is, Chomsky claims, no factual question as to whether any such decision is right or
wrong—just as there is no question as to whether our decision to say that airplanes fly is right, or our decision not to
say that ships swim is wrong. However, this seems to oversimplify matters. The important question is, Could it ever be
appropriate to say that computers think and, if so, what conditions must a computer satisfy in order to be so described?

Some authors offer the Turing test as a definition of intelligence. However, the mathematician and logician Alan Turing
himself pointed out that a computer that ought to be described as intelligent might nevertheless fail his test if it
were incapable of successfully imitating a human being. For example, ChatGPT often invokes its status as a large
language model and thus would be unlikely to pass the Turing test. If an intelligent entity can fail the test, then the
test cannot function as a definition of intelligence. It is even questionable whether passing the test would actually
show that a computer is intelligent, as the information theorist Claude Shannon and the AI pioneer John McCarthy pointed
out in 1956. Shannon and McCarthy argued that, in principle, it is possible to design a machine containing a complete
set of canned responses to all the questions that an interrogator could possibly ask during the fixed time span of the
test. Like PARRY, this machine would produce answers to the interviewer’s questions by looking up appropriate responses
in a giant table. This objection seems to show that, in principle, a system with no intelligence at all could pass the
Turing test.

In fact, AI has no real definition of intelligence to offer, not even in the subhuman case. Rats are intelligent, but
what exactly must an artificial intelligence achieve before researchers can claim that it has reached rats’ level of
success? In the absence of a reasonably precise criterion for when an artificial system counts as intelligent, there is
no objective way of telling whether an AI research program has succeeded or failed. One result of AI’s failure to
produce a satisfactory criterion of intelligence is that, whenever researchers achieve one of AI’s goals—for example, a
program that can hold a conversation like GPT or beat the world chess champion like Deep Blue—critics are able to say,
“That’s not intelligence!” Marvin Minsky’s response to the problem of defining intelligence is to maintain—like Turing
before him—that intelligence is simply our name for any problem-solving mental process that we do not yet understand.
Minsky likens intelligence to the concept of “unexplored regions of Africa”: it disappears as soon as we discover it.

Want to learn more?
• Find out why AI messes up hands and fingers.

Abstract vector hi speed internet technology background
More From Britannica
History of Technology Timeline
• How much do you know about computers and technology?

• What are the major events in 21st-century technology?

B.J. Copeland
computer programming language
Table of Contents
Introduction
Language types
Elements of programming
References & Edit History
Related Topics
Videos
Human trafficking
For Students

programming language summary
Quizzes
computer chip. computer. Hand holding computer chip. Central processing unit (CPU). history and society, science and
technology, microchip, microprocessor motherboard computer Circuit Board
Computers and Technology Quiz
Read Next
Programming computer abstract
Influential Computer Programming Languages
Discover
Hindu Holi Festival celebrations with colored water, powder and colorful flower petals thrown over celebrants at a Hindu
temple in Mathura, Uttar Pradesh, India on March 24, 2021.
Holi: Festival of Colors
Alexandria, Egypt - September 12, 2014 - Wedding Bridesmaids Competition, reward for Best Bridesmaids for participation
in competitions. Prize for victory
Who Votes for the Academy Awards?
pg 229Nazi parade features a banner proclaiming, "Death to Marxism."The possibility of a peaceful Germany after World
War I was precluded entirely by the terms of the Versailles Treaty and theintransigent hostility of France and England.
Stripped of indu
Were the Nazis Socialists?
Los Angeles Police Department wanted flyer on Elizabeth Short, aka the "Black Dahlia," who was brutally murdered in
January 1947. The FBI supported the Los Angeles Police Department in the case, including by identifying Short through
her fingerprints that
America’s 5 Most Notorious Cold Cases (Including One You May Have Thought Was Already Solved)
Shadow of a man holding large knife in his hand inside of some dark, spooky buiding
7 of History's Most Notorious Serial Killers
Shah Jahan. Taj Mahal. Mughal architecture. Emperor Shah Jahan fifth Mughal Emperor (reigned 1628-1658) India, Himachal
Pradesh, Basohli or Jammu and Kashmir, Mankot, circa 1690 Drawings; Opaque watercolor, gold, and ink on paper (see
notes)
6 Important Mughal Emperors
A Factory Interior, watercolor, pen and gray ink, graphite, and white goache on wove paper by unknown artist, c.
1871-91; in the Yale Center for British Art. Industrial Revolution England
Inventors and Inventions of the Industrial Revolution
Technology
Computers
computer programming language
Written by
Fact-checked by
Article History
Key People: Stephen Wolfram Niklaus Emil Wirth Kristen Nygaard John Warner Backus Alan Kay
Related Topics: PHP list of programming languages Python CSS Go
computer programming language, any of various languages for expressing a set of detailed instructions for a digital
computer. Such instructions can be executed directly when they are in the computer manufacturer-specific numerical form
known as machine language, after a simple substitution process when expressed in a corresponding assembly language, or
after translation from some “higher-level” language. Although there are many computer languages, relatively few are
widely used.

Machine and assembly languages are “low-level,” requiring a programmer to manage explicitly all of a computer’s
idiosyncratic features of data storage and operation. In contrast, high-level languages shield a programmer from
worrying about such considerations and provide a notation that is more easily written and read by programmers.

Language types
Machine and assembly languages
A machine language consists of the numeric codes for the operations that a particular computer can execute directly. The
codes are strings of 0s and 1s, or binary digits (“bits”), which are frequently converted both from and to hexadecimal
(base 16) for human viewing and modification. Machine language instructions typically use some bits to represent
operations, such as addition, and some to represent operands, or perhaps the location of the next instruction. Machine
language is difficult to read and write, since it does not resemble conventional mathematical notation or human
language, and its codes vary from computer to computer.

Assembly language is one level above machine language. It uses short mnemonic codes for instructions and allows the
programmer to introduce names for blocks of memory that hold data. One might thus write “add pay, total” instead of
“0110101100101000” for an instruction that adds two numbers.

Assembly language is designed to be easily translated into machine language. Although blocks of data may be referred to
by name instead of by their machine addresses, assembly language does not provide more sophisticated means of organizing
complex information. Like machine language, assembly language requires detailed knowledge of internal computer
architecture. It is useful when such details are important, as in programming a computer to interact with peripheral
devices (printers, scanners, storage devices, and so forth).

computer chip. computer. Hand holding computer chip. Central processing unit (CPU). history and society, science and
technology, microchip, microprocessor motherboard computer Circuit Board
Britannica Quiz
Computers and Technology Quiz
Algorithmic languages
Algorithmic languages are designed to express mathematical or symbolic computations. They can express algebraic
operations in notation similar to mathematics and allow the use of subprograms that package commonly used operations for
reuse. They were the first high-level languages.

FORTRAN
The first important algorithmic language was FORTRAN (formula translation), designed in 1957 by an IBM team led by John
Backus. It was intended for scientific computations with real numbers and collections of them organized as one- or
multidimensional arrays. Its control structures included conditional IF statements, repetitive loops (so-called DO
loops), and a GOTO statement that allowed nonsequential execution of program code. FORTRAN made it convenient to have
subprograms for common mathematical operations, and built libraries of them.


Get Unlimited Access
Try Britannica Premium for free and discover more.
FORTRAN was also designed to translate into efficient machine language. It was immediately successful and continues to
evolve.

ALGOL
ALGOL (algorithmic language) was designed by a committee of American and European computer scientists during 1958–60 for
publishing algorithms, as well as for doing computations. Like LISP (described in the next section), ALGOL had recursive
subprograms—procedures that could invoke themselves to solve a problem by reducing it to a smaller problem of the same
kind. ALGOL introduced block structure, in which a program is composed of blocks that might contain both data and
instructions and have the same structure as an entire program. Block structure became a powerful tool for building large
programs out of small components.

ALGOL contributed a notation for describing the structure of a programming language, Backus–Naur Form, which in some
variation became the standard tool for stating the syntax (grammar) of programming languages. ALGOL was widely used in
Europe, and for many years it remained the language in which computer algorithms were published. Many important
languages, such as Pascal and Ada (both described later), are its descendants.

C
The C programming language was developed in 1972 by Dennis Ritchie and Brian Kernighan at the AT&T Corporation for
programming computer operating systems. Its capacity to structure data and programs through the composition of smaller
units is comparable to that of ALGOL. It uses a compact notation and provides the programmer with the ability to operate
with the addresses of data as well as with their values. This ability is important in systems programming, and C shares
with assembly language the power to exploit all the features of a computer’s internal architecture. C, along with its
descendant C++, remains one of the most common languages.

Business-oriented languages
COBOL
COBOL (common business oriented language) has been heavily used by businesses since its inception in 1959. A committee
of computer manufacturers and users and U.S. government organizations established CODASYL (Committee on Data Systems and
Languages) to develop and oversee the language standard in order to ensure its portability across diverse systems.

COBOL uses an English-like notation—novel when introduced. Business computations organize and manipulate large
quantities of data, and COBOL introduced the record data structure for such tasks. A record clusters heterogeneous
data—such as a name, an ID number, an age, and an address—into a single unit. This contrasts with scientific languages,
in which homogeneous arrays of numbers are common. Records are an important example of “chunking” data into a single
object, and they appear in nearly all modern languages.

SQL
SQL (structured query language) is a language for specifying the organization of databases (collections of records).
Databases organized with SQL are called relational, because SQL provides the ability to query a database for information
that falls in a given relation. For example, a query might be “find all records with both last name Smith and city New
York.” Commercial database programs commonly use an SQL-like language for their queries.

Education-oriented languages
BASIC
BASIC (beginner’s all-purpose symbolic instruction code) was designed at Dartmouth College in the mid-1960s by John
Kemeny and Thomas Kurtz. It was intended to be easy to learn by novices, particularly non-computer science majors, and
to run well on a time-sharing computer with many users. It had simple data structures and notation and it was
interpreted: a BASIC program was translated line-by-line and executed as it was translated, which made it easy to locate
programming errors.

Its small size and simplicity also made BASIC a popular language for early personal computers. Its recent forms have
adopted many of the data and control structures of other contemporary languages, which makes it more powerful but less
convenient for beginners.

Pascal
About 1970 Niklaus Wirth of Switzerland designed Pascal to teach structured programming, which emphasized the orderly
use of conditional and loop control structures without GOTO statements. Although Pascal resembled ALGOL in notation, it
provided the ability to define data types with which to organize complex information, a feature beyond the capabilities
of ALGOL as well as FORTRAN and COBOL. User-defined data types allowed the programmer to introduce names for complex
data, which the language translator could then check for correct usage before running a program.

During the late 1970s and ’80s, Pascal was one of the most widely used languages for programming instruction. It was
available on nearly all computers, and, because of its familiarity, clarity, and security, it was used for production
software as well as for education.

Logo
Logo originated in the late 1960s as a simplified LISP dialect for education; Seymour Papert and others used it at MIT
to teach mathematical thinking to schoolchildren. It had a more conventional syntax than LISP and featured “turtle
graphics,” a simple method for generating computer graphics. (The name came from an early project to program a
turtlelike robot.) Turtle graphics used body-centred instructions, in which an object was moved around a screen by
commands, such as “left 90” and “forward,” that specified actions relative to the current position and orientation of
the object rather than in terms of a fixed framework. Together with recursive routines, this technique made it easy to
program intricate and attractive patterns.

Hypertalk
Hypertalk was designed as “programming for the rest of us” by Bill Atkinson for Apple’s Macintosh. Using a simple
English-like syntax, Hypertalk enabled anyone to combine text, graphics, and audio quickly into “linked stacks” that
could be navigated by clicking with a mouse on standard buttons supplied by the program. Hypertalk was particularly
popular among educators in the 1980s and early ’90s for classroom multimedia presentations. Although Hypertalk had many
features of object-oriented languages (described in the next section), Apple did not develop it for other computer
platforms and let it languish; as Apple’s market share declined in the 1990s, a new cross-platform way of displaying
multimedia left Hypertalk all but obsolete (see the section World Wide Web display languages).

Object-oriented languages
Object-oriented languages help to manage complexity in large programs. Objects package data and the operations on them
so that only the operations are publicly accessible and internal details of the data structures are hidden. This
information hiding made large-scale programming easier by allowing a programmer to think about each part of the program
in isolation. In addition, objects may be derived from more general ones, “inheriting” their capabilities. Such an
object hierarchy made it possible to define specialized objects without repeating all that is in the more general ones.

Object-oriented programming began with the Simula language (1967), which added information hiding to ALGOL. Another
influential object-oriented language was Smalltalk (1980), in which a program was a set of objects that interacted by
sending messages to one another.

C++
The C++ language, developed by Bjarne Stroustrup at AT&T in the mid-1980s, extended C by adding objects to it while
preserving the efficiency of C programs. It has been one of the most important languages for both education and
industrial programming. Large parts of many operating systems were written in C++. C++, along with Java, has become
popular for developing commercial software packages that incorporate multiple interrelated applications. C++ is
considered one of the fastest languages and is very close to low-level languages, thus allowing complete control over
memory allocation and management. This very feature and its many other capabilities also make it one of the most
difficult languages to learn and handle on a large scale.

C#
C# (pronounced C sharp like the musical note) was developed by Anders Hejlsberg at Microsoft in 2000. C# has a syntax
similar to that of C and C++ and is often used for developing games and applications for the Microsoft Windows operating
system.

Ada
Ada was named for Augusta Ada King, countess of Lovelace, who was an assistant to the 19th-century English inventor
Charles Babbage, and is sometimes called the first computer programmer. Ada, the language, was developed in the early
1980s for the U.S. Department of Defense for large-scale programming. It combined Pascal-like notation with the ability
to package operations and data into independent modules. Its first form, Ada 83, was not fully object-oriented, but the
subsequent Ada 95 provided objects and the ability to construct hierarchies of them. While no longer mandated for use in
work for the Department of Defense, Ada remains an effective language for engineering large programs.

Java
In the early 1990s Java was designed by Sun Microsystems, Inc., as a programming language for the World Wide Web (WWW).
Although it resembled C++ in appearance, it was object-oriented. In particular, Java dispensed with lower-level
features, including the ability to manipulate data addresses, a capability that is neither desirable nor useful in
programs for distributed systems. In order to be portable, Java programs are translated by a Java Virtual Machine
specific to each computer platform, which then executes the Java program. In addition to adding interactive capabilities
to the Internet through Web “applets,” Java has been widely used for programming small and portable devices, such as
mobile telephones.

Visual Basic
Visual Basic was developed by Microsoft to extend the capabilities of BASIC by adding objects and “event-driven”
programming: buttons, menus, and other elements of graphical user interfaces (GUIs). Visual Basic can also be used
within other Microsoft software to program small routines. Visual Basic was succeeded in 2002 by Visual Basic .NET, a
vastly different language based on C#, a language with similarities to C++.

Python
The open-source language Python was developed by Dutch programmer Guido van Rossum in 1991. It was designed as an
easy-to-use language, with features such as using indentation instead of brackets to group statements. Python is also a
very compact language, designed so that complex jobs can be executed with only a few statements. In the 2010s, Python
became one of the most popular programming languages, along with Java and JavaScript.

Declarative languages
Declarative languages, also called nonprocedural or very high level, are programming languages in which (ideally) a
program specifies what is to be done rather than how to do it. In such languages there is less difference between the
specification of a program and its implementation than in the procedural languages described so far. The two common
kinds of declarative languages are logic and functional languages.

Logic programming languages, of which PROLOG (programming in logic) is the best known, state a program as a set of
logical relations (e.g., a grandparent is the parent of a parent of someone). Such languages are similar to the SQL
database language. A program is executed by an “inference engine” that answers a query by searching these relations
systematically to make inferences that will answer a query. PROLOG has been used extensively in natural language
processing and other AI programs.

Functional languages have a mathematical style. A functional program is constructed by applying functions to arguments.
Functional languages, such as LISP, ML, and Haskell, are used as research tools in language development, in automated
mathematical theorem provers, and in some commercial projects.

computer chip. computer. Hand holding computer chip. Central processing unit (CPU). history and society, science and
technology, microchip, microprocessor motherboard computer Circuit Board
Britannica Quiz
Computers and Technology Quiz
Scripting languages
Scripting languages are sometimes called little languages. They are intended to solve relatively small programming
problems that do not require the overhead of data declarations and other features needed to make large programs
manageable. Scripting languages are used for writing operating system utilities, for special-purpose file-manipulation
programs, and, because they are easy to learn, sometimes for considerably larger programs.

Perl was developed in the late 1980s, originally for use with the UNIX operating system. It was intended to have all the
capabilities of earlier scripting languages. Perl provided many ways to state common operations and thereby allowed a
programmer to adopt any convenient style. In the 1990s it became popular as a system-programming tool, both for small
utility programs and for prototypes of larger ones. Together with other languages discussed below, it also became
popular for programming computer Web “servers.”

Document formatting languages
Document formatting languages specify the organization of printed text and graphics. They fall into several classes:
text formatting notation that can serve the same functions as a word processing program, page description languages that
are interpreted by a printing device, and, most generally, markup languages that describe the intended function of
portions of a document.

TeX
TeX was developed during 1977–86 as a text formatting language by Donald Knuth, a Stanford University professor, to
improve the quality of mathematical notation in his books. Text formatting systems, unlike WYSIWYG (“What You See Is
What You Get”) word processors, embed plain text formatting commands in a document, which are then interpreted by the
language processor to produce a formatted document for display or printing. TeX marks italic text, for example, as {\it
this is italicized}, which is then displayed as this is italicized.

TeX largely replaced earlier text formatting languages. Its powerful and flexible abilities gave an expert precise
control over such things as the choice of fonts, layout of tables, mathematical notation, and the inclusion of graphics
within a document. It is generally used with the aid of “macro” packages that define simple commands for common
operations, such as starting a new paragraph; LaTeX is a widely used package. TeX contains numerous standard “style
sheets” for different types of documents, and these may be further adapted by each user. There are also related programs
such as BibTeX, which manages bibliographies and has style sheets for all of the common bibliography styles, and
versions of TeX for languages with various alphabets.

PostScript
PostScript is a page-description language developed in the early 1980s by Adobe Systems Incorporated on the basis of
work at Xerox PARC (Palo Alto Research Center). Such languages describe documents in terms that can be interpreted by a
personal computer to display the document on its screen or by a microprocessor in a printer or a typesetting device.

PostScript commands can, for example, precisely position text, in various fonts and sizes, draw images that are
mathematically described, and specify colour or shading. PostScript uses postfix, also called reverse Polish notation,
in which an operation name follows its arguments. Thus, “300 600 20 270 arc stroke” means: draw (“stroke”) a 270-degree
arc with radius 20 at location (300, 600). Although PostScript can be read and written by a programmer, it is normally
produced by text formatting programs, word processors, or graphic display tools.

The success of PostScript is due to its specification’s being in the public domain and to its being a good match for
high-resolution laser printers. It has influenced the development of printing fonts, and manufacturers produce a large
variety of PostScript fonts.

SGML
SGML (standard generalized markup language) is an international standard for the definition of markup languages; that
is, it is a metalanguage. Markup consists of notations called tags that specify the function of a piece of text or how
it is to be displayed. SGML emphasizes descriptive markup, in which a tag might be “<emphasis>.” Such a markup denotes
    the document function, and it could be interpreted as reverse video on a computer screen, underlining by a
    typewriter, or italics in typeset text.

    SGML is used to specify DTDs (document type definitions). A DTD defines a kind of document, such as a report, by
    specifying what elements must appear in the document—e.g., <Title>—and giving rules for the use of document
        elements, such as that a paragraph may appear within a table entry but a table may not appear within a
        paragraph. A marked-up text may be analyzed by a parsing program to determine if it conforms to a DTD. Another
        program may read the markups to prepare an index or to translate the document into PostScript for printing. Yet
        another might generate large type or audio for readers with visual or hearing disabilities.

        World Wide Web display languages
        HTML
        The World Wide Web is a system for displaying text, graphics, and audio retrieved over the Internet on a
        computer monitor. Each retrieval unit is known as a Web page, and such pages frequently contain “links” that
        allow related pages to be retrieved. HTML (hypertext markup language) is the markup language for encoding Web
        pages. It was designed by Tim Berners-Lee at the CERN nuclear physics laboratory in Switzerland during the 1980s
        and is defined by an SGML DTD. HTML markup tags specify document elements such as headings, paragraphs, and
        tables. They mark up a document for display by a computer program known as a Web browser. The browser interprets
        the tags, displaying the headings, paragraphs, and tables in a layout that is adapted to the screen size and
        fonts available to it.

        HTML documents also contain anchors, which are tags that specify links to other Web pages. An anchor has the
        form <A HREF=“http://www.britannica.com”> Encyclopædia Britannica</A>, where the quoted string is the URL
        (uniform resource locator) to which the link points (the Web “address”) and the text following it is what
        appears in a Web browser, underlined to show that it is a link to another page. What is displayed as a single
        page may also be formed from multiple URLs, some containing text and others graphics.

        XML
        HTML does not allow one to define new text elements; that is, it is not extensible. XML (extensible markup
        language) is a simplified form of SGML intended for documents that are published on the Web. Like SGML, XML uses
        DTDs to define document types and the meanings of tags used in them. XML adopts conventions that make it easy to
        parse, such as that document entities are marked by both a beginning and an ending tag, such as <BEGIN>…</BEGIN>
        . XML provides more kinds of hypertext links than HTML, such as bidirectional links and links relative to a
        document subsection.

        laptop computer
        More From Britannica
        computer science: Programming languages
        Because an author may define new tags, an XML DTD must also contain rules that instruct a Web browser how to
        interpret them—how an entity is to be displayed or how it is to generate an action such as preparing an e-mail
        message.

        Web scripting
        Web pages marked up with HTML or XML are largely static documents. Web scripting can add information to a page
        as a reader uses it or let the reader enter information that may, for example, be passed on to the order
        department of an online business. CGI (common gateway interface) provides one mechanism; it transmits requests
        and responses between the reader’s Web browser and the Web server that provides the page. The CGI component on
        the server contains small programs called scripts that take information from the browser system or provide it
        for display. A simple script might ask the reader’s name, determine the Internet address of the system that the
        reader uses, and print a greeting. Scripts may be written in any programming language, but, because they are
        generally simple text-processing routines, scripting languages like PERL are particularly appropriate.

        Another approach is to use a language designed for Web scripts to be executed by the browser. JavaScript is one
        such language, designed by the Netscape Communications Corp., which may be used with both Netscape’s and
        Microsoft’s browsers. JavaScript is a simple language, quite different from Java. A JavaScript program may be
        embedded in a Web page with the HTML tag
        <script language=“JavaScript”>. JavaScript instructions following that tag will be executed by the browser when the page is selected.In order to speed up display of dynamic(interactive) pages, JavaScript is often combined with XML or some other language for exchanging information between the server and the client’s browser.In particular, the XMLHttpRequest command enables asynchronous data requests from the server without requiring the server to resend the entire Web page.This approach, or “philosophy,” of programming is called Ajax(asynchronous JavaScript and XML).

VB Script is a subset of Visual Basic.Originally developed for Microsoft’s Office suite of programs, it was later used for Web scripting as well.Its capabilities are similar to those of JavaScript, and it may be embedded in HTML in the same fashion.

Behind the use of such scripting languages for Web programming lies the idea of component programming, in which programs are constructed by combining independent previously written components without any further language processing.JavaScript and VB Script programs were designed as components that may be attached to Web browsers to control how they display information.

Elements of programming
Despite notational differences, contemporary computer languages provide many of the same programming structures.These include basic control structures and data structures.The former provide the means to express algorithms, and the latter provide ways to organize information.

Control structures
Programs written in procedural languages, the most common kind, are like recipes, having lists of ingredients and step - by - step instructions for using them.The three basic control structures in virtually every procedural language are:

            1. Sequence—combine the liquid ingredients, and next add the dry ones.
2. Conditional—if the tomatoes are fresh then simmer them, but if canned, skip this step.
3. Iterative—beat the egg whites until they form soft peaks.
Sequence is the default control structure; instructions are executed one after another.They might, for example, carry out a series of arithmetic operations, assigning results to variables, to find the roots of a quadratic equation ax2 + bx + c = 0. The conditional IF - THEN or IF - THEN - ELSE control structure allows a program to follow alternative paths of execution.Iteration, or looping, gives computers much of their power.They can repeat a sequence of steps as often as necessary, and appropriate repetitions of quite simple steps can solve complex problems.

These control structures can be combined.A sequence may contain several loops; a loop may contain a loop nested within it, or the two branches of a conditional may each contain sequences with loops and more conditionals.In the “pseudocode” used in this article, “*” indicates multiplication and “←” is used to assign values to variables.The following programming fragment employs the IF - THEN structure for finding one root of the quadratic equation, using the quadratic formula:

quadratic formula.

The quadratic formula assumes that a is nonzero and that the discriminant(the portion within the square root sign) is not negative(in order to obtain a real number root).Conditionals check those assumptions:

IF a = 0 THEN
            ROOT ← −c / b
            ELSE
            DISCRIMINANT ← b * b − 4 * a * c
IF DISCRIMINANT ≥ 0 THEN
            ROOT ← (−b + SQUARE_ROOT(DISCRIMINANT))/2*a
            ENDIF
            ENDIF
The SQUARE_ROOT function used in the above fragment is an example of a subprogram(also called a procedure, subroutine, or function).A subprogram is like a sauce recipe given once and used as part of many other recipes.Subprograms take inputs(the quantity needed) and produce results(the sauce).Commonly used subprograms are generally in a collection or library provided with a language.Subprograms may call other subprograms in their definitions, as shown by the following routine(where ABS is the absolute - value function).SQUARE_ROOT is implemented by using a WHILE (indefinite) loop that produces a good approximation for the square root of real numbers unless x is very small or very large.A subprogram is written by declaring its name, the type of input data, and the output:

FUNCTION SQUARE_ROOT(REAL x) RETURNS REAL
            ROOT ← 1.0
WHILE ABS(ROOT * ROOT − x) ≥ 0.000001
AND WHILE ROOT ← (x / ROOT + ROOT) / 2
RETURN ROOT
Subprograms can break a problem into smaller, more tractable subproblems.Sometimes a problem may be solved by reducing it to a subproblem that is a smaller version of the original.In that case the routine is known as a recursive subprogram because it solves the problem by repeatedly calling itself.For example, the factorial function in mathematics(n! = n∙(n−1)⋯3∙2∙1—i.e., the product of the first n integers), can be programmed as a recursive routine:

FUNCTION FACTORIAL(INTEGER n) RETURNS INTEGER
IF n = 0 THEN RETURN 1
ELSE RETURN n * FACTORIAL(n−1)
The advantage of recursion is that it is often a simple restatement of a precise definition, one that avoids the bookkeeping details of an iterative solution.

At the machine - language level, loops and conditionals are implemented with branch instructions that say “jump to” a new point in the program.The “goto” statement in higher - level languages expresses the same operation but is rarely used because it makes it difficult for humans to follow the “flow” of a program.Some languages, such as Java and Ada, do not allow it.

Data structures
Whereas control structures organize algorithms, data structures organize information.In particular, data structures specify types of data, and thus which operations can be performed on them, while eliminating the need for a programmer to keep track of memory addresses.Simple data structures include integers, real numbers, Booleans(true / false), and characters or character strings.Compound data structures are formed by combining one or more data types.

The most important compound data structures are the array, a homogeneous collection of data, and the record, a heterogeneous collection.An array may represent a vector of numbers, a list of strings, or a collection of vectors(an array of arrays, or mathematical matrix).A record might store employee information—name, title, and salary.An array of records, such as a table of employees, is a collection of elements, each of which is heterogeneous.Conversely, a record might contain a vector—i.e., an array.

Record components, or fields, are selected by name; for example, E.SALARY might represent the salary field of record E.An array element is selected by its position or index; A[10] is the element at position 10 in array A.A FOR loop(definite iteration) can thus run through an array with index limits(FIRST TO LAST in the following example) in order to sum its elements:

FOR i ← FIRST TO LAST
            SUM ← SUM + A[i]
Arrays and records have fixed sizes.Structures that can grow are built with dynamic allocation, which provides new storage as required.These data structures have components, each containing data and references to further components(in machine terms, their addresses).Such self - referential structures have recursive definitions.A bintree(binary tree) for example, either is empty or contains a root component with data and left and right bintree “children.” Such bintrees implement tables of information efficiently.Subroutines to operate on them are naturally recursive; the following routine prints out all the elements of a bintree(each is the root of some subtree):

PROCEDURE TRAVERSE(ROOT: BINTREE)
IF NOT(EMPTY(ROOT))
            TRAVERSE(ROOT.LEFT)
PRINT ROOT.DATA
            TRAVERSE(ROOT.RIGHT)
            ENDIF
Abstract data types(ADTs) are important for large - scale programming.They package data structures and operations on them, hiding internal details.For example, an ADT table provides insertion and lookup operations to users while keeping the underlying structure, whether an array, list, or binary tree, invisible.In object - oriented languages, classes are ADTs and objects are instances of them.The following object - oriented pseudocode example assumes that there is an ADT bintree and a “superclass” COMPARABLE, characterizing data for which there is a comparison operation(such as “<” for integers).It defines a new ADT, TABLE, that hides its data - representation and provides operations appropriate to tables.This class is polymorphic—defined in terms of an element - type parameter of the COMPARABLE class. Any instance of it must specify that type, here a class with employee data(the COMPARABLE declaration means that PERS_REC must provide a comparison operation to sort records).Implementation details are omitted.

CLASS TABLE OF < COMPARABLE T >
                PRIVATE DATA: BINTREE OF < T >
                    PUBLIC INSERT(ITEM: T)
PUBLIC LOOKUP(ITEM: T) RETURNS BOOLEAN
            END
CLASS PERS_REC: COMPARABLE
PRIVATE NAME: STRING
PRIVATE POSITION: { STAFF, SUPERVISOR, MANAGER }
PRIVATE SALARY: REAL
PUBLIC COMPARE(R: PERS_REC) RETURNS BOOLEAN
            END
            EMPLOYEES: TABLE < PERS_REC >
                TABLE makes public only its own operations; thus, if it is modified to use an array or list rather than a bintree, programs that use it cannot detect the change.This information hiding is essential to managing complexity in large programs.It divides them into small parts, with “contracts” between the parts; here the TABLE class contracts to provide lookup and insertion operations, and its users contract to use only the operations so publicized.

David Hemmendinger
computer network
Table of Contents
            Introduction
            References & Edit History
Related Topics
            Images & Videos
Fiber optic cables connected to a computer network server
What is the dark web ?
                The architecture of a networked information system
            Quizzes
Internet http://www blue screen. Hompepage blog 2009, history and society, media news television, crowd opinion protest, In the News 2009, breaking news
What Do You Actually Know About the Internet ?
                Related Questions
Who were key people involved with the creation of the Internet ?
                Who controls the Internet ?
                    Read Next
Binary Computer Code, Binary Code, Internet, Technology, Password, Data
How Does Wi - Fi Work ?
                Internet http://www blue screen. Hompepage blog 2009, history and society, media news television, crowd opinion protest, In the News 2009, breaking news
Who Invented the Internet ?
                Abstract vector hi speed internet technology background
History of Technology Timeline
Screen with https for internet security. (encryption, privacy, websites)
Inventions that Helped Shape How We Interact with Knowledge and Information
multiple exposures, depiction of deep web, internet
            What’s the Difference Between the Deep Web and the Dark Web ?
                Discover
Small, white rat(genus Rattus) on a glass table. (rodent, laboratory, experiment)
Cruel and Unusual Punishments: 15 Types of Torture
Iraqi Army Soldiers from the 9th Mechanized Division learning to operate and maintain M1A1 Abrams Main Battle Tanks at Besmaya Combat Training Center, Baghdad, Iraq, 2011. Military training.Iraq war.U.S.Army
            8 Deadliest Wars of the 21st Century
            Jackson(Wyoming, United States).Jackson Lake(also called Jackson Hole), southern end of the Teton Range(the Grand Tetons), Grand Teton National Park, Wyoming, USA
            7 Wonders of America
Calendar showing the month of February
Why Are There Only 28 Days in February ?
                Adolf Hitler(Nazi, nazism, German leader).
9 Things You Might Not Know About Adolf Hitler
Hindu Holi Festival celebrations with colored water, powder and colorful flower petals thrown over celebrants at a Hindu temple in Mathura, Uttar Pradesh, India on March 24, 2021.
            Holi: Festival of Colors
Leonardo da Vinci(1452 - 1519), Italian Renaissance painter from Florence.Engraving by Cosomo Colombini(d. 1812) after a Leonardo self portrait.Ca. 1500.
            10 Famous Artworks by Leonardo da Vinci
            Technology
The Web & Communication
Fiber optic cables connected to a computer network server
Fiber optic cables connected to a computer network server A behind - the - scenes look at a computer network reveals fiber optic cables(bathed in blue and yellow light) connected to a server.
computer network
Written and fact - checked by 
Last Updated: Feb 15, 2025 • Article History
Key People: Vinton Cerf Lawrence Roberts Douglas Engelbart Leonard Kleinrock Geoffrey Hinton
Related Topics: Internet ARPANET e - commerce social network virtual community
computer network, two or more computers that are connected to each other to communicate data electronically.Besides physically connecting computer and communication devices, a network system serves the important function of establishing a cohesive architecture that allows a variety of equipment types to transfer information in a near - seamless fashion.Two popular architectures are ISO Open Systems Interconnection(OSI) and IBM’s Systems Network Architecture(SNA).

What is the dark web ?
                What is the dark web ? Learn about the dark web.It has a high profile but makes up a minuscule portion of the Internet.
See all videos for this article
Two basic network types are local area networks(LANs) and wide area networks(WANs).LANs connect computers and peripheral devices in a limited physical area, such as a business office, laboratory, or college campus, by means of links(wires, Ethernet cables, fiber optics, or Wi - Fi) that transmit data rapidly.A typical LAN consists of two or more personal computers, printers, and high - capacity disk - storage devices called file servers, which enable each computer on the network to access a common set of files.LAN operating system software, which interprets input and instructs networked devices, allows users to communicate with each other, share the printers and storage equipment, and simultaneously access centrally located processors, data, or programs(instruction sets).LAN users may also access other LANs or tap into WANs.LANs with similar architectures are linked by “bridges,” which act as transfer points.LANs with different architectures are linked by “gateways,” which convert data as they pass between systems.

WANs connect computers and smaller networks to larger networks over greater geographic areas, including different continents.They may link the computers by means of cables, fiber optics, or satellites, but their users commonly access the networks via a modem(a device that allows computers to communicate over telephone lines).The largest WAN is the Internet, a collection of networks and gateways linking billions of computer users on every continent.

Internet http://www blue screen. Hompepage blog 2009, history and society, media news television, crowd opinion protest, In the News 2009, breaking news
Britannica Quiz
What Do You Actually Know About the Internet ?
                The Editors of Encyclopaedia Britannica
This article was most recently revised and updated by Tara Ramanathan.
computer security
Table of Contents
            Introduction & Top Questions
            References & Edit History
Quick Facts & Related Topics
            Videos
This video demonstrates how blockchain tech works.
cybersecurity CEO
cybersecurity service director
Related Questions
What is computer security ?
                Does artificial intelligence(AI) compromise computer secutiry ?
                    Is Internet technology "making us stupid" ?
                        What is the impact of artificial intelligence(AI) technology on society ?
                            What is a computer ?
                                Read Next
Internet http://www blue screen. Hompepage blog 2009, history and society, media news television, crowd opinion protest, In the News 2009, breaking news
Who Invented the Internet ?
                Technical insides of a desktop computer
            5 Components of Information Systems
Programming computer abstract
Influential Computer Programming Languages
Abstract vector hi speed internet technology background
History of Technology Timeline
Binary Computer Code, Binary Code, Internet, Technology, Password, Data
How Does Wi - Fi Work ?
                Discover
pg 229Nazi parade features a banner proclaiming, "Death to Marxism."The possibility of a peaceful Germany after World War I was precluded entirely by the terms of the Versailles Treaty and theintransigent hostility of France and England.Stripped of indu
Were the Nazis Socialists ?
                Statue of Nostradamus
Nostradamus and His Prophecies
A Factory Interior, watercolor, pen and gray ink, graphite, and white goache on wove paper by unknown artist, c. 1871 - 91; in the Yale Center for British Art.Industrial Revolution England
Inventors and Inventions of the Industrial Revolution
Groups of depositors in front of the closed American Union Bank, New York City.April 26, 1932. Great Depression run on bank crowd
Causes of the Great Depression
Los Angeles Police Department wanted flyer on Elizabeth Short, aka the "Black Dahlia," who was brutally murdered in January 1947. The FBI supported the Los Angeles Police Department in the case, including by identifying Short through her fingerprints that
            America’s 5 Most Notorious Cold Cases(Including One You May Have Thought Was Already Solved)
African Americans demonstrating for voting rights in front of the White House as police and others watch, March 12, 1965. One sign reads, "We demand the right to vote everywhere." Voting Rights Act, civil rights.
Timeline of the American Civil Rights Movement
Sailboat against a beautiful landscape
Which Waters Do You Pass Through When You “Sail the Seven Seas”?
            Technology
            Engineering
Civil Engineering
computer security
Also known as: cyber security, cybersecurity
Written and fact - checked by 
Last Updated: Feb 14, 2025 • Article History
Also called: cybersecurity
Related Topics: computer security and protection system password VPN
Top Questions
What is computer security ?
                Does artificial intelligence(AI) compromise computer secutiry ?
                    News • Scientists create world's 1st chip that can protect data in the age of quantum computing attacks • Feb. 25, 2025, 4:50 AM ET (Live Science)
computer security, the protection of computer systems and information from harm, theft, and unauthorized use.Computer hardware is typically protected by the same means used to protect other valuable or sensitive equipment—namely, serial numbers, doors and locks, and alarms.The protection of information and system access, on the other hand, is achieved through other tactics, some of them quite complex.

The security precautions related to computer information and access address four major threats: (1) theft of data, such as that of military secrets from government computers; (2) vandalism, including the destruction of data by a computer virus; (3) fraud, such as employees at a bank channeling funds into their own accounts; and(4) invasion of privacy, such as the illegal accessing of protected personal financial or medical data from a large database.The most basic means of protecting a computer system against theft, vandalism, invasion of privacy, and other irresponsible behaviours is to electronically track and record the access to, and activities of, the various users of a computer system.This is commonly done by assigning an individual password to each person who has access to a system.The computer system itself can then automatically track the use of these passwords, recording such data as which files were accessed under particular passwords and so on.Another security measure is to store a system’s data on a separate device or medium that is normally inaccessible through the computer system.Finally, data is often encrypted so that it can be deciphered only by holders of a singular encryption key. (See data encryption.)

Computer security has become increasingly important since the late 1960s, when modems(devices that allow computers to communicate over telephone lines) were introduced.The proliferation of personal computers in the 1980s compounded the problem because they enabled hackers(irresponsible computerphiles) to illegally access major computer systems from the privacy of their homes.With the tremendous growth of the Internet in the late 20th and early 21st centuries, computer security became a widespread concern.The development of advanced security techniques aims to diminish such threats, though concurrent refinements in the methods of computer crime pose ongoing hazards.

The Editors of Encyclopaedia Britannica
This article was most recently revised and updated by Erik Gregersen.
                hardware
Table of Contents
            Introduction
            References & Edit History
Related Topics
            Quizzes
computer chip.computer.Hand holding computer chip.Central processing unit(CPU).history and society, science and technology, microchip, microprocessor motherboard computer Circuit Board
Computers and Technology Quiz
Related Questions
Is Internet technology "making us stupid" ?
                What is the impact of artificial intelligence(AI) technology on society ?
                    What is a computer ?
                        Who invented the computer ?
                            What can computers do?
                                Read Next
Technical insides of a desktop computer
            5 Components of Information Systems
Internet http://www blue screen. Hompepage blog 2009, history and society, media news television, crowd opinion protest, In the News 2009, breaking news
Who Invented the Internet ?
                Programming computer abstract
Influential Computer Programming Languages
Abstract vector hi speed internet technology background
History of Technology Timeline
molecule model, element
Molecular Machines: Making for the Future
Discover
Jackson(Wyoming, United States).Jackson Lake(also called Jackson Hole), southern end of the Teton Range(the Grand Tetons), Grand Teton National Park, Wyoming, USA
            7 Wonders of America
pg 229Nazi parade features a banner proclaiming, "Death to Marxism."The possibility of a peaceful Germany after World War I was precluded entirely by the terms of the Versailles Treaty and theintransigent hostility of France and England.Stripped of indu
Were the Nazis Socialists ?
                African Americans demonstrating for voting rights in front of the White House as police and others watch, March 12, 1965. One sign reads, "We demand the right to vote everywhere." Voting Rights Act, civil rights.
Timeline of the American Civil Rights Movement
Iraqi Army Soldiers from the 9th Mechanized Division learning to operate and maintain M1A1 Abrams Main Battle Tanks at Besmaya Combat Training Center, Baghdad, Iraq, 2011. Military training.Iraq war.U.S.Army
            8 Deadliest Wars of the 21st Century
Black widow spider
            9 of the World’s Deadliest Spiders
            Small, white rat(genus Rattus) on a glass table. (rodent, laboratory, experiment)
Cruel and Unusual Punishments: 15 Types of Torture
Ice Sledge Hockey, Hockey Canada Cup, USA(left) vs Canada, 2009. UBC Thunderbird Arena, Vancouver, BC, competition site for Olympic ice hockey and Paralympic ice sledge hockey.Vancouver 2010 Olympic and Paralympic Winter Games, Vancouver Olympics
            10 Best Hockey Players of All Time
            Technology
            Computers
            hardware
            computing
Written and fact - checked by 
Article History
Related Topics: computer
            hardware, computer machinery and equipment, including memory, cabling, power supply, peripheral devices, and circuit boards.Computer operation requires both hardware and software.Hardware design specifies a computer’s capability; software instructs the computer on what to do.The advent of microprocessors in the late 1970s led to much smaller hardware assemblies and accelerated the proliferation of computers.Personal computers and mobile devices such as smartphones and tablet computers have become as powerful as the early mainframes, while servers and mainframes are now smaller and have vastly more computing power than the early models.

The Editors of Encyclopaedia Britannica
This article was most recently revised and updated by Erik Gregersen.
operating system
Table of Contents
            Introduction
            References & Edit History
Quick Facts & Related Topics
            Images
iPhone 6Figure 4: The role of the operating system.
For Students

operating system summary
            Quizzes
computer chip.computer.Hand holding computer chip.Central processing unit(CPU).history and society, science and technology, microchip, microprocessor motherboard computer Circuit Board
Computers and Technology Quiz
Technician operates the system console on the new UNIVAC 1100 / 83 computer at the Fleet Analysis Center, Corona Annex, Naval Weapons Station, Seal Beach, CA.June 1, 1981. Univac magnetic tape drivers or readers in background.Universal Automatic Computer
Computers and Operating Systems
Read Next
Technical insides of a desktop computer
            5 Components of Information Systems
President Richard M.Nixon smiles and gives the victory sign as he boards the White House helicopter after his resignation on August 9, 1974 in Washington, D.C.
Timeline of the 1970s
            Discover
Los Angeles Police Department wanted flyer on Elizabeth Short, aka the "Black Dahlia," who was brutally murdered in January 1947. The FBI supported the Los Angeles Police Department in the case, including by identifying Short through her fingerprints that
            America’s 5 Most Notorious Cold Cases(Including One You May Have Thought Was Already Solved)
Statue of Nostradamus
Nostradamus and His Prophecies
            Alexandria, Egypt - September 12, 2014 - Wedding Bridesmaids Competition, reward for Best Bridesmaids for participation in competitions.Prize for victory
Who Votes for the Academy Awards ?
                    Hindu Holi Festival celebrations with colored water, powder and colorful flower petals thrown over celebrants at a Hindu temple in Mathura, Uttar Pradesh, India on March 24, 2021.
            Holi: Festival of Colors
            Jackson(Wyoming, United States).Jackson Lake(also called Jackson Hole), southern end of the Teton Range(the Grand Tetons), Grand Teton National Park, Wyoming, USA
            7 Wonders of America
Shah Jahan.Taj Mahal.Mughal architecture.Emperor Shah Jahan fifth Mughal Emperor(reigned 1628 - 1658) India, Himachal Pradesh, Basohli or Jammu and Kashmir, Mankot, circa 1690 Drawings; Opaque watercolor, gold, and ink on paper(see notes)
            6 Important Mughal Emperors
pg 229Nazi parade features a banner proclaiming, "Death to Marxism."The possibility of a peaceful Germany after World War I was precluded entirely by the terms of the Versailles Treaty and theintransigent hostility of France and England.Stripped of indu
Were the Nazis Socialists ?
                Technology
Computers
iPhone 6
iPhone 6 The iPhone 6, released in 2014.
operating system
            computing
Also known as: OS
Written by
            Fact - checked by 
Last Updated: Feb 15, 2025 • Article History
Key People: Bill Gates Steve Ballmer Linus Torvalds Fred Brooks Richard Stallman
Related Topics: iOS Microsoft Windows IBM OS / 2 macOS Palm OS
On the Web: Stanford University - Operating Systems(PDF)(Feb. 15, 2025)
operating system(OS), program that manages a computer’s resources, especially the allocation of those resources among other programs.Typical resources include the central processing unit(CPU), computer memory, file storage, input / output(I / O) devices, and network connections.Management tasks include scheduling resource use to avoid conflicts and interference between programs.Unlike most programs, which complete a task and terminate, an operating system runs indefinitely and terminates only when the computer is turned off.

Modern multiprocessing operating systems allow many processes to be active, where each process is a “thread” of computation being used to execute a program.One form of multiprocessing is called time - sharing, which lets many users share computer access by rapidly switching between them.Time - sharing must guard against interference between users’ programs, and most systems use virtual memory, in which the memory, or “address space,” used by a program may reside in secondary memory(such as on a magnetic hard disk drive) when not in immediate use, to be swapped back to occupy the faster main computer memory on demand.This virtual memory both increases the address space available to a program and helps to prevent programs from interfering with each other, but it requires careful control by the operating system and a set of allocation tables to keep track of memory use.Perhaps the most delicate and critical task for a modern operating system is allocation of the CPU; each process is allowed to use the CPU for a limited time, which may be a fraction of a second, and then must give up control and become suspended until its next turn.Switching between processes must itself use the CPU while protecting all data of the processes.

The first digital computers had no operating systems.They ran one program at a time, which had command of all system resources, and a human operator would provide any special resources needed.The first operating systems were developed in the mid - 1950s.These were small “supervisor programs” that provided basic I / O operations(such as controlling punch card readers and printers) and kept accounts of CPU usage for billing.Supervisor programs also provided multiprogramming capabilities to enable several programs to run at once.This was particularly important so that these early multimillion - dollar machines would not be idle during slow I / O operations.

Technician operates the system console on the new UNIVAC 1100 / 83 computer at the Fleet Analysis Center, Corona Annex, Naval Weapons Station, Seal Beach, CA.June 1, 1981. Univac magnetic tape drivers or readers in background.Universal Automatic Computer
Britannica Quiz
Computers and Operating Systems
Computers acquired more powerful operating systems in the 1960s with the emergence of time - sharing, which required a system to manage multiple users sharing CPU time and terminals.Two early time - sharing systems were CTSS(Compatible Time Sharing System), developed at the Massachusetts Institute of Technology, and the Dartmouth College Basic System, developed at Dartmouth College.Other multiprogrammed systems included Atlas, at the University of Manchester, England, and IBM’s OS / 360, probably the most complex software package of the 1960s.After 1972 the Multics system for General Electric Co.’s GE 645 computer(and later for Honeywell Inc.’s computers) became the most sophisticated system, with most of the multiprogramming and time - sharing capabilities that later became standard.

The minicomputers of the 1970s had limited memory and required smaller operating systems.The most important operating system of that period was UNIX, developed by AT & T for large minicomputers as a simpler alternative to Multics.It became widely used in the 1980s, in part because it was free to universities and in part because it was designed with a set of tools that were powerful in the hands of skilled programmers.More recently, Linux, an open - source version of UNIX developed in part by a group led by Finnish computer science student Linus Torvalds and in part by a group led by American computer programmer Richard Stallman, has become popular on personal computers as well as on larger computers.

In addition to such general - purpose systems, special - purpose operating systems run on small computers that control assembly lines, aircraft, and even home appliances.They are real - time systems, designed to provide rapid response to sensors and to use their inputs to control machinery.Operating systems have also been developed for mobile devices such as smartphones and tablets.Apple Inc.’s iOS, which runs on iPhones and iPads, and Google Inc.’s Android are two prominent mobile operating systems.

From the standpoint of a user or an application program, an operating system provides services.Some of these are simple user commands like “dir”—show the files on a disk—while others are low - level “system calls” that a graphics program might use to display an image.In either case the operating system provides appropriate access to its objects, the tables of disk locations in one case and the routines to transfer data to the screen in the other.Some of its routines, those that manage the CPU and memory, are generally accessible only to other portions of the operating system.


Get Unlimited Access
Try Britannica Premium for free and discover more.
Contemporary operating systems for personal computers commonly provide a graphical user interface(GUI).The GUI may be an intrinsic part of the system, as in the older versions of Apple’s Mac OS and Microsoft Corporation’s Windows OS; in others it is a set of programs that depend on an underlying system, as in the X Window system for UNIX and Apple’s Mac OS X.

Operating systems also provide network services and file - sharing capabilities—even the ability to share resources between systems of different types, such as Windows and UNIX.Such sharing has become feasible through the introduction of network protocols(communication rules) such as the Internet’s TCP / IP.

David Hemmendinger
</p>
<div id="test">The history of computers goes back over 200 years. At first theorized by mathematicians and entrepreneurs, during the
19th century mechanical calculating machines were designed and built to solve the increasingly complex number-crunching
challenges. The advancement of technology enabled ever more-complex computers by the early 20th century, and computers
became larger and more powerful.

Today, computers are almost unrecognizable from designs of the 19th century, such as Charles Babbage's Analytical Engine
— or even from the huge computers of the 20th century that occupied whole rooms, such as the Electronic Numerical
Integrator and Calculator.

Here's a brief history of computers, from their primitive number-crunching origins to the powerful modern-day machines
that surf the Internet, run games and stream multimedia.
</div>









    
</body>
</html>